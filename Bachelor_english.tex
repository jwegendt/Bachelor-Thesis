%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% LaTeX-Rahmen fuer das Erstellen von englischen Bachelorarbeiten
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% allgemeine Einstellungen
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[twoside,12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
%\usepackage{reportpage}
\usepackage{epsf}
\usepackage{graphics, graphicx}
\usepackage{latexsym}
\usepackage[margin=10pt,font=small,labelfont=bf]{caption}

%\usepackage{fontspec}
%\setmainfont{Asana-Math}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{array} %for the \newcolumntype macro
\usepackage{amsthm} %for theorems/definitions
\usepackage{thmtools} %for listofheorems TODO: remove/integrate?
\usepackage{bussproofs}

\textwidth 14cm
\textheight 22cm
\topmargin 0.0cm
\evensidemargin 1cm
\oddsidemargin 1cm
%\footskip 2cm
\parskip0.5explus0.1exminus0.1ex

% Kann von Student auch nach persönlichem Geschmack verändert werden.
\pagestyle{headings}

\usepackage[symbol]{footmisc}
\usepackage{hyperref}
\usepackage{cleveref}

\sloppy

\begin{document}


%%%%% Macros %TODO: sollten die wo anders hin/in ein eigenes file?


%Macros for Theorems/Definitions/...
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{example}{Example}[section]

%Macros for possible terms in ND:
\newcommand{\constructor}{K(\overline{e})}
\newcommand{\destructor}{e.d(\overline{e}) }
\newcommand{\patmatch}{e.\textbf{case}\ \{\overline{K(\overline{x})\Rightarrow t}\}} 
\newcommand{\copatmatch}{\textbf{cocase}\ \{\overline{d(\overline{x})\Rightarrow t}\}}

%Macros for terms but for examples
\newcommand{\exconstructor}[1]{K(#1)}
\newcommand{\exdestructor}[1]{e.d(#1) }
\newcommand{\expatmatch}[1]{\textbf{case}\ \{#1\}} 
\newcommand{\excopatmatch}[1]{\textbf{cocase}\ \{#1\}}

%Macros for structural/technical latex stuff
\newcolumntype{L}{>{$}l<{$}} % a left-aligned column in mathmode
\newcolumntype{C}{>{$}c<{$}} % a center-aligned column in mathmode

\newcommand{\alphaunifvar}{\alpha^?}
\newcommand{\betaunifvar}{\beta^?}
\newcommand{\FV}[1]{\text{FV}(#1)}
\newcommand{\listofexpr}{e_1,...e_n}
\newcommand{\listofvar}{x_1,...x_n}
\newcommand{\betaconv}{\equiv_{\beta}^1}
\newcommand{\etaconv}{\equiv_{\eta}^1}
\newcommand{\ap}[1]{\texttt{ap}(#1)}

\newcommand{\partition}{\;|\;}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\True}{\mathtt{True}}
\newcommand{\False}{\mathtt{False}}
\newcommand{\Nil}{\mathtt{Nil}}

\newcommand{\Int}{\mathsf{Int}}
\newcommand{\Bool}{\mathsf{Bool}}

\newcommand{\constraints}{\mathcal{C}}

\newcommand{\mathsfbrackets}[2]{\mathsf{#1(}#2\mathsf{)}}
\newcommand{\mathttbrackets}[2]{\mathtt{#1(}#2\mathtt{)}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Layout Title page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\begin{titlepage}
 \begin{center}
  {\LARGE Eberhard Karls Universität Tübingen}\\
  {\large Mathematisch-Naturwissenschaftliche Fakultät \\
Wilhelm-Schickard-Institut für Informatik\\[4cm]}
  {\huge Bachelor Thesis Computer Science\\[2cm]}
  {\Large\bf  Higher-Order Unification for Data and Codata Types\\[1.5cm]}
 {\large Julia Wegendt}\\[0.5cm]
Date\\[3cm]
{\small\bf Reviewer}\\[0.5cm]
 {\large Prof. Dr. Klaus Ostermann}\\
  {\footnotesize Department of Computer Science\\
	University of Tübingen}
\end{center}
	
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Layout back of title page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty}
\vspace*{\fill}
\begin{minipage}{11.2cm}
\textbf{Wegendt, Julia}\\
\emph{Higher-Order Unification for Data and Codata Types}\\ Bachelor Thesis Computer Science\\
Eberhard Karls Universität Tübingen\\
Period: from-till
\end{minipage}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{roman}
\setcounter{page}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Abstract}

Algorithms for solving different subproblems of the general higher-order unification problem have been discussed in the literature.
I present an untyped calculus with a focus on the data-codata duality, 
and discuss typing for this calculus.

Furthermore, I go over the decidable subproblem of first-order unification and an accompanying algorithm adapted to our syntax.
I explain higher-order unification and the specific problem of pattern unification formally, 
and discuss how they differ, especially what parts are decidable.

Finally, I present a framework for our algorithm, as well as our unification algorithm for the aforementioned calculus. 
This algorithm covers pattern unification problems, as well as problems which can be reduced to pattern unification problems.


\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Acknowledgements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}

Write here your acknowledgements.

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Table of Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.3}
\small\normalsize

\tableofcontents

\renewcommand{\baselinestretch}{1}
\small\normalsize

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Main Part
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{arabic}
\setcounter{page}{1}

\section{Introduction}

In this thesis, I want to present a higher-order unification algorithm for a calculus with codata and data.. 
For this, you need to know what data and codata conceptually are, how they are presented in our syntax and how they function.
You also need to know what higher-order unification is and how it differs from the first-order unification you probably already know.
These are the goals for the next few paragraphs, after which I then give a quick overview of the different parts of this thesis.

\subsection{Data and codata}
While algebraic data are a well-known concept, their brother codata is less talked about. 
Codata is dual to data, which (roughly) means that you can express any concept with codata that you would with data.
On a high level: While data is concerned with how values are constructed or put in, codata is concerned with how values are destructed or taken out. 
For instance, look at these different ways to define the same tuple: 

\begin{example}[Two ways to define the same tuple]
    \begin{align*}
        &\mathttbrackets{Tup}{\mathtt{1,5}} \tag*{Data}\\
        &\excopatmatch{\mathtt{fst \Rightarrow 1}, \mathtt{snd \Rightarrow 5}} \tag*{Codata}
    \end{align*}     
    While the first variant focuses on what we put into the tuple, the second focuses on how we can take its components out of it.   
\end{example}

One form of codata often talked about is streams:
\begin{example}[Streams]
    \begin{align*}
        Trues = \excopatmatch{\mathtt{hd \Rightarrow True, tl \Rightarrow} Trues}
    \end{align*}
    This is the stream that contains an infinite amount of Trues.
    Also notice again that there is an emphasis on how we can take things out of our stream, rather than how it is constructed.
    As you can see, we can also instantiate infinite codata. 
\end{example}
In our syntax, codata is also used for function application, 
which is the part of codata you are likely most familiar with:

\begin{example}[Function application through codata]
    \begin{align*}
        id = \excopatmatch{\ap{x}\Rightarrow x}
    \end{align*}
    This is the identity function, which we get from mapping the function applicator $\ap{}$ to the same variable we apply it to.
\end{example}
    
\subsection{Higher-Order Unification}
Most people probably first encounter unification problems in school, when solving simple math problems.
Unification is also necessary in a few important applications in computer science: 
Type checking and type inference, in proof assistants, as well as more particular fields like logic programming and computational linguistics.

While unification means solving equations with unknown variables, there are two major types of unification: First-order unification and higher-order unification.
First-order unification is enough for the type systems in most typed programming languages. It is only when more complex types are introduced, 
like in the interactive theorem prover Coq or in the programming language Agda, that higher-order unification is required.
% Yes, this is not completely old->new, but i want to create a strong dichotomy between first and higher order unification, 
% and want the reader to keep both concepts in mind at the same time => i repeat them and contrast them twice.

\begin{example}[First-order unification problems]
    \begin{align*}
        &\mathtt{5} \equiv \alphaunifvar\tag{1}\\
        &\mathtt{True \equiv False}\tag{2}\\
        &\mathsf{List(\alphaunifvar)\equiv List(\betaunifvar)}\tag{3}\\
        &\mathtt{Tup(1,\alphaunifvar)}\equiv \betaunifvar \tag{4}
    \end{align*}
    Each of the given equations can be seen as their own unification problem.
    The first has an obvious solution $[\mathtt{5/\alphaunifvar}]$, so substituting $\mathtt{5}$ for $\alphaunifvar$ (even though there are multiple solutions even here).
    The second is a unification problem without unification variables, and has no solution.
    The third and forth are both unification problems with two unification variables, and have many solutions, but one most general solution.
\end{example}
Note that unification problems may also have multiple equations.

\begin{example}[Higher-order unification problems]\label{sec:mgu}
    In this first example, we are looking for a function that when applied to $\mathtt{5}$, evaluates to $\mathtt{5}$:
    \begin{align*}
        &\alphaunifvar.\ap{\mathtt{5}} \equiv \mathtt{5}\tag{1} 
    \end{align*}
    We can think of two obvious solutions for this: The identity function: $[id / \alphaunifvar]$, and the constant function which always returns \texttt{5}: $[\excopatmatch{\ap{x}\Rightarrow \mathtt{5}} / \alphaunifvar]$.
    As already implied before, sometimes our unification problems have multiple solutions, 
    but we are interested in one particular solution. This is the solution that is more general, the one from which the others can be obtained by instantiation.
    In this case however, there is no one such solution. This can only happen in higher-order unification, but this will be explored more formally below.
    Another example of higher-order unification:
    \begin{align*}
    \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False \Rightarrow 2}}\equiv \mathtt{1} \tag{2}
    \end{align*}
    Here we have an example of pattern matching implementing an if-clause.
    If we substituted $[\mathtt{True / \alphaunifvar}]$, the left side would reduce to $\mathtt{1}$,
    since the first case matches --- solving the equation.
\end{example}

\subsection{Overview}
This Bachelors thesis is structured as follows:

First, I will explore a calculus with a heavy focus on the dualities between data and codata in \cref{sec:The Untyped Calculus ND}.
I will give examples and show how we can use this syntax to define everything we want to define, and point out how the duality plays out in our syntax.
In addition, I will introduce some concepts we need so that we can start to play around with terms.

In \cref{sec:Simple Types}, I give examples of how we can type terms we have seen in our examples, 
and present a typing system to categorize many terms with simple types.

To make our way to higher-order unification, I will first discuss first-order unification in \cref{sec:First-Order Unification}.
I explain the problem formally and give an algorithm to solve it.

Then I explain higher-order unification (\cref{sec:Higher-Order Unification}) formally, as well as the pattern unification problem which is easier to solve.

In \cref{The Algorithm For Higher-Order Unification}, I present the framework for our equations, 
and then present our algorithm for higher-order unification problems written in our calculus.
The algorithm is made up of two parts, which I go though in more detail, and describe how they fit together.

In the end, there is a section about related works (\cref{sec:Related works}), as well as a small conclusion in \cref{sec:conclusion}.

\section{The Untyped Calculus ND}\label{sec:The Untyped Calculus ND}

I will introduce the Untyped Calculus ND, based on \cite{binder2024programming}.
This calculus may be seen as an extension to the lambda calculus, as function definition is only one application of codata in this calculus,
meaning we can do more than just function application! 

\subsection{Syntax of the Untyped Calculus ND}\label{sec:syntax}

I write $\overline{e}$ to mean a (possibly empty) list of expressions: $\overline{e}= e_1, ..., e_n$.  
Constructors $\constructor$ and destructors $\destructor$ both get such a list of expressions and construct or destruct terms.
A pattern match $\patmatch$ matches an expression $e$ against a sequence of clauses, where each clause consists of a constructor and an term $t$.
For a copattern match $\copatmatch$
similar rules apply, but the clauses contain destructors instead of constructors.

\begin{definition}[Terms of the Calculus ND]
    \begin{align*}
    e,r,s,t ::&=  x,y  \tag*{Variable} \\
        &\partition\constructor \tag*{Constructor} \\
        &\partition\patmatch  \tag*{Pattern match}\\
        &\partition\copatmatch  \tag*{Copattern match}\\
        &\partition\destructor  \tag*{Destructor}
    \end{align*}
    In pattern and copattern matching, every constructor or destructor may occur no more than once.
    This is so we don't match multiple clauses:
    In $e.\expatmatch{\mathttbrackets{Tup}{x_1, x_2}\Rightarrow t_1, \mathttbrackets{Tup}{x, y}\Rightarrow t_2}$, the same tuple could be reduced to two different terms, $t_1, t_2$.
\end{definition}

I use different fonts for different uses of our syntax: 
%$x, y \in Var, \constructor \in \textsc{CtorDtorName} \lor \textsc{TypeName}, \destructor \in\textsc{CtorDtorName}.$
Variables, and names of terms are in $VarFont$. When constructors and destructors are used for data and codata, they appear in $\mathtt{CtorDtorFont}$, 
but when constructors are used for types, they appear in $\mathsf{TypeFont}$.

\subsection{How is this used?}

To get familiar with the syntax and to understand the underlying semantics, 
I will go through some examples and explain what they implement.

We use constructors for types:
\begin{example}[Constructing types with constructors]
    Simple Types like $\mathsf{Int, Bool, ...}$ are constructors applied to an empty sequence, 
    while composite types of course require arguments.
    \begin{align*}
        &\mathsf{Bool}\tag{1}\\
        &\mathsf{List(Int)}\tag{2}\\
        &\mathsf{Pair(Int, Bool)}\tag{3}
    \end{align*}
    Types are explored later on in \cref{sec:Simple Types}.
\end{example}

We of course also want to build terms for those data types. 
Some terms we build with constructors:

\begin{example}[Instantiating data with constructors]
    \begin{align*}
        &\texttt{True, False}\tag{1}\\
        &\mathtt{Tup(1,2)}\tag{2}\\
        &\mathtt{Date(27, 08, 2005)}\tag{3}\\
        &\mathtt{Cons(True, Cons(False, Nil))}\tag{4}              
    \end{align*}
    As you can see from the last example, we can use constructors to define recursive data types.
    Numbers for instance, are also implemented recursively as Peano numbers.
    I use numerals $(\mathtt{1, 2, 5, ...})$ as a shorthand for $\mathtt{suc(zero), suc(suc(zero)), ...}$ throughout this thesis, though.
\end{example}

If we construct terms, we might also want a way to take them apart. 
For the terms we just saw, this can be done using pattern matching:

\begin{example}[Taking apart data using pattern matching]
    \begin{align*}
        &\True.\expatmatch{\mathtt{True\Rightarrow False, False\Rightarrow True}}\tag{1}\\
        &\mathtt{Tup(1,2)}.\expatmatch{\mathttbrackets{Tup}{x, y}\Rightarrow x}\tag{2}\\
        &\mathtt{Cons(True, Cons(False, Nil))}.\expatmatch{\mathttbrackets{Cons}{x, y}\Rightarrow x}\tag{3}   
    \end{align*}
    This is the pattern matching you probably already know. 
    We will see in \cref{sec:betaconv} how these get evaluated to $\False$, $\mathtt{1}$ and $\True$, respectively.
\end{example}

Since we use pattern matching to deconstruct data like Booleans, naturally we can also use it to implement conditionals:
\begin{example}[Representing conditionals with pattern matching]
    \begin{align*}
        &t.\expatmatch{\True\Rightarrow e_1, \False\Rightarrow e_2} \tag{1}\\
        &e.\expatmatch{\mathttbrackets{Cons}{\True, x}\Rightarrow \True, 
        \mathtt{Nil\Rightarrow False}, \mathttbrackets{Cons}{x, y}\Rightarrow\False}\tag{2}
    \end{align*}
The first example is equivalent to if $t$ then $e_1$ else $e_2$. The second expression tests whether a given list starts with $\True$.
\end{example}

Until now, we have seen how to instantiate data, and how to take it apart. 
Now this is where codata comes into play ---this is how to instantiate it:

\begin{example}[Instantiating codata with copattern matching]
    \begin{align*}
        &\excopatmatch{\mathtt{fst\Rightarrow 1, snd\Rightarrow 2}}\tag{1}\\
        Trues =& \excopatmatch{\mathtt{hd \Rightarrow True, tl\Rightarrow}Trues}\tag{2} 
    \end{align*}
    We have seen examples like this in the introduction; the first one is a tuple with the values $\mathtt{1}$ and $\mathtt{2}$.
    The second example is the infinite stream of $Trues$.
\end{example}

Now for how to take instantiations of codata apart:

\begin{example}[Taking apart codata using destructors]
    \begin{align*}
        &\excopatmatch{\mathtt{fst\Rightarrow 1, snd\Rightarrow 2}}.\mathtt{fst}\tag{1}\\
        &Trues.\mathtt{hd}\tag{2}\\
        &id.\ap{\mathtt{5}}\tag{3}
    \end{align*}
    These examples do exactly what you think they do: We select the first value of the tuple, 
    take out the first element of our stream, and apply the identity function on $\mathtt{5}$, respectively.
    We will see how these examples resolve to $\mathtt{1}$, $\True$ and $\mathtt{5}$ in \cref{sec:betaconv}.
\end{example} 

You might have already noticed that there is no option for function definition in the syntax of our calculus.
This is because we can use copattern matching to implement function definitions:
\begin{example}[Defining functions using copattern matching]
    \begin{align*}
        &id = \excopatmatch{\ap{x}\Rightarrow x}\tag{1}\\
        &\excopatmatch{\ap{x}\Rightarrow \excopatmatch{\ap{y}\Rightarrow x}} \tag{2} \\
        &\excopatmatch{\ap{x}\Rightarrow \excopatmatch{\ap{y}\Rightarrow y}} \tag{3}
    \end{align*}
    The first example implements the identity function, which you have already seen. 
    The second and third examples implement functions that take two arguments, ignore one and give back the other.
\end{example}

\subsection{Dualities in data and codata}
You might have wondered why in the syntax definition, copatterns are defined first, even though they need destructors to function, which are defined only afterwards.
The reason becomes clear when you reread the titles of the examples:
Constructors are used to instantiate data, while pattern matching is used to take apart data.
Copattern matching is used to instantiate codata, while destructors are used to take apart codata.
The order of the syntax definition emphasizes these conceptual dualitites:
Data is dual to codata, but this is only because constructors are dual to copattern matching, 
and pattern matching is dual to destructors.
Here is one more clean example to drive this point home:  

\begin{example}[Dualities in a tuple defined in two ways]
    \begin{align*}
        \resizebox{\columnwidth}{!}{ %to rescale the table so that its width is the width of our column
            \begin{tabular}{|l | l | l|}
                \hline
                & data & codata\\
                \hline
                instantiate & $\mathtt{Tup(1,2)}$&  $\excopatmatch{\mathtt{fst\Rightarrow 1, snd\Rightarrow 2}}$\\
                take apart & $.\expatmatch{\mathttbrackets{Tup}{x, y}\Rightarrow x}$ & $.\mathtt{fst}$\\
                example & $\mathtt{Tup(1,2)}.\expatmatch{\mathttbrackets{Tup}{x, y}\Rightarrow x}$&$\excopatmatch{\mathtt{fst\Rightarrow 1, snd\Rightarrow 2}}.\mathtt{fst}$\\
               \hline
           \end{tabular}
        }
    \end{align*}        
\end{example}

\subsection{Free Variables, Substitutions, Contexts}

\begin{definition}[Free Variables]
    The set of free variables of a term $e$ is FV($e$). A term is closed if this set is empty.
    Free Variables are defined recursively over the structure of terms as follows:
    \begin{align*}
        \begin{split}
            \FV{x} &:= \{x\}\\
            \FV{\exconstructor{\listofexpr}} &:= \FV{e_1} \cup \dots\cup \FV{e_n}\\
            \FV{\patmatch} &:= \FV{e}\cup (\FV{t_1}\backslash(\overline{x})_1)\cup \dots\cup (\FV{t_n}\backslash(\overline{x})_n)\\
            \FV{\copatmatch} &:= (\FV{t_1}\backslash(\overline{x})_1)\cup \dots\cup (\FV{t_n}\backslash(\overline{x})_n)\\
            \FV{\exdestructor{e_1,...e_n}} &:= \FV{e}\cup FV(e_1) \cup \dots \cup \FV{e_n}    
        \end{split}
    \end{align*}
%TODO: explanation
\end{definition}

\begin{definition}[Substitution]
    A simultaneous substitution $\sigma$ of the terms $\listofexpr$ for the distinct variables $\listofvar$ is defined as follows:
    \begin{align*} %TODO: geht das mit dem distinct da oben unter?
        \sigma ::= [\listofexpr \backslash\listofvar]
    \end{align*}
\end{definition}

\begin{definition}[Domain and Range of a Substitution]
    The definitions of Domain and Range of a Substitution are as follows:
    \begin{align*}
        \texttt{dom}([\listofexpr/\listofvar]) &:= \{\listofvar\}\\
        \texttt{rng}([\listofexpr/\listofvar]) &:= \FV{e_1}\cup \dots\cup\FV{e_n}
    \end{align*}
    The domain is the set of variables for which the substitution is defined, and
    the range is the set of free variables which appear in the substitution. 
\end{definition}

The interesting thing however, is what happens when we apply a substitution to an expression:

\begin{definition}[Action of a Substitution]
    The action of a substitution $\sigma$ on a term $e$, written as $e \sigma$ and is defined as follows:
    \begin{align*}
        x[\listofexpr / \listofvar] :=& e_i \quad(\text{if } x=x_i) \\
        y\sigma :=& y\quad (\text{if }y\notin \texttt{dom}(\sigma))\\
        (\exconstructor{\listofexpr})\sigma :=& \exconstructor{e_1 \sigma,\dots , e_n \sigma}\\
        (\expatmatch{\overline{K(\overline{x})\Rightarrow e}})\sigma:=& (e\sigma).\textbf{case}\{\overline{K(\overline{y})\Rightarrow (e\sigma')\sigma}\}\\
        (\excopatmatch{\overline{d(\overline{x})\Rightarrow e}})\sigma:=& \excopatmatch{\overline{d(\overline{y})\Rightarrow(e\sigma')\sigma}}\\
        (\exdestructor{\listofexpr}) \sigma:=& (e \sigma).d(e_1 \sigma,\dots , e_n \sigma)
    \end{align*} 
    Where $\sigma'$ is a substitution that ensures that we don't bind new variables: 
    $\sigma'$ has the form $[y_1,\dots, y_n/\listofvar]$ and all $y_i$ are fresh for both the domain and the range of $\sigma$.
\end{definition}


Sometimes we want to attach multiple substitutions together.
This process is called composition of substitutions and looks like this: $\sigma = \sigma_2 \circ \sigma_1$, which is equivalent to first applying the substitution $\sigma_1$, then the substitution $\sigma_2$.

\begin{definition}[Composition of Subsitutions]
    Given two substitutions
    \begin{align*}
        \sigma_1 := [\listofexpr/\listofvar],\qquad \sigma_2 := [t_1,\dots,t_m/y_1,\dots,y_m],
    \end{align*}
    composition is defined as:
    \begin{align*}
        \sigma_2\circ\sigma_1 := [e_1\sigma_2,\dots,e_n\sigma_2,t_j,\dots,t_k/\listofvar,y_j,\dots,y_k]
    \end{align*}
    Where $j, \dots, k$ ist the greatest sub-range of indices $1,\dots,m$ such that none of the variables $y_j$ to $y_k$ is in the domain of $\sigma_1$. 
\end{definition}

\begin{definition}[Idempotency]
    A substitution $\sigma$ is idempotent, iff. $\sigma \circ \sigma = \sigma$.
    Concretely, this means that it doesn't matter how often we apply a substitution to a given expression.
\end{definition}

Consider these two examples. One of an idempotent substitution, and one of a substitution that is not idempotent:
\begin{example}[Idempotency]
    $[\excopatmatch{\ap{x}\Rightarrow x} / y]$ is idempotent, since:
    \begin{align*}
        &[\excopatmatch{\ap{x}\Rightarrow x} / y] \circ [\excopatmatch{\ap{x}\Rightarrow x} / y] \tag{1}\\
        =& [\excopatmatch{\ap{x}\Rightarrow x}[\excopatmatch{\ap{x}\Rightarrow x} / y] / y] \\
        =& [\excopatmatch{\ap{x}\Rightarrow x} / y] 
    \end{align*}
On the other hand, the substitution $[\excopatmatch{\ap{y}\Rightarrow x} / x]$ is not idempotent, since:
\begin{align*}
    &[\excopatmatch{\ap{y}\Rightarrow x} / x] \circ [\excopatmatch{\ap{y}\Rightarrow x} / x]\tag{2}\\ 
    =& [\excopatmatch{\ap{y}\Rightarrow x}[\excopatmatch{\ap{y}\Rightarrow x} / x] / x] \\
    =& \excopatmatch{\ap{y}\Rightarrow (\ap{y}\Rightarrow x) } \neq [\excopatmatch{\ap{y}\Rightarrow x} / x]
\end{align*}
\end{example}

\begin{definition}[More general]\label{more general}
    A substitution $\sigma$ is more general than a substitution $\theta$, iff. there exists a mapping $\tau$, such that: $\theta = \tau \circ \sigma$.
\end{definition}

For example, in the following unification problem, we are trying to substitute types for two unification variables:
\begin{example}[More general]
    \begin{align*}
        \mathsf{List}(\alphaunifvar) &\equiv \mathsf{List}(\betaunifvar)\\
        \alphaunifvar &\equiv \mathsf{Int}
    \end{align*}
    One solution might be: $\theta = [\mathsf{Int, Int / \alphaunifvar, \betaunifvar}]$, 
so substituting $\mathsf{Int}$ for both unification variables.
The more general solution is $\sigma = [\mathsf{Int,\alphaunifvar / \alphaunifvar, \betaunifvar}]$, however.
This is because there exists a mapping $\tau = [\mathsf{Int / \alphaunifvar}]$, such that:
\begin{align*}
    \tau \circ \sigma &= [\mathsf{Int / \alphaunifvar}]\circ[\mathsf{Int,\alphaunifvar / \alphaunifvar, \betaunifvar}] \\
    &= [\mathsf{Int[Int/\alphaunifvar], \alphaunifvar[Int/\alphaunifvar] / \alphaunifvar, \betaunifvar]= [Int, Int / \alphaunifvar, \betaunifvar]= \theta}
\end{align*}    
\end{example}
%TODO: das ist ein vorgriff! diese sektion doch weiter runter schieben und neues beispiel?

\subsection{Conversion}

\begin{definition}[Beta-conversion] \label{sec:betaconv}
    A single step of beta-conversion $e_1 \betaconv e_2$ is defined as follows:
    \begin{align*}
        \excopatmatch{\dots,d(\overline{x}) \Rightarrow e, \dots}.d(\overline{e})
        \betaconv & e[\overline{e}/\overline{x}]  
        \tag{$\beta$-codata}\\
        \constructor.\textbf{case}\{\dots,\exconstructor{\overline{x}}\Rightarrow e,\dots\}
        \betaconv & e[\overline{e}/\overline{x}] 
        \tag{$\beta$-data}
    \end{align*}
    We require that the constructor $K(\overline{e})$ and the constructor $\exconstructor{\overline{x}}$ have the same number of arguments.
    This, in short ensures that we don't generate stuck terms (terms that can't be evaluated).
\end{definition}
Consider these examples of beta-conversion:
\begin{example}[Beta-conversion]
    \begin{align*}
        \excopatmatch{\ap{x}\Rightarrow \True}.\ap{x}&\betaconv \True[x/x] = \True 
        \tag{1}\\
        \excopatmatch{\ap{y}\Rightarrow y}.\ap{x} &\betaconv y[x/y]= x
        \tag{2}\\
        \True.\expatmatch{\False\Rightarrow \True, \True \Rightarrow \False}&\betaconv\False
        \tag{3}\\
        \excopatmatch{\mathtt{fst\Rightarrow 1, snd\Rightarrow 2}}.\mathtt{fst}\betaconv \mathtt{1}
        \tag{4}
    \end{align*}
    Intuitively, beta-conversion means not only function application but also the reduction under pattern or copattern matching. %TODO: macht das so sinn? sollte ich das mehr erklären? sind die beispiele hier überhaupt gut?        
\end{example}

\begin{definition}[Eta-Conversion for codata]
    A single step of eta-conversion $e_1 \etaconv e_2$ is defined as follows:
    \begin{align*}
        \excopatmatch{\overline{d(\overline{x})\Rightarrow e.d(\overline{x})}}
        \etaconv & e \quad (\text{if } \overline{x}\notin \FV{e}) \tag{$\eta$-codata}
    \end{align*}
    The expression $e$ needs to be the same in all the different clauses!
\end{definition}

Now consider these examples of eta-conversion:
\begin{example}[Eta-conversion for codata]
    \begin{align*}
        \excopatmatch{\ap{y}\Rightarrow id.\ap{y}}&\etaconv id\tag{1}\\
        \excopatmatch{\mathtt{fst\Rightarrow} specificPair.\mathtt{fst}, \mathtt{snd\Rightarrow}specificPair.\mathtt{snd}}&\etaconv specificPair \tag{2}\\      
        \text{where } specificPair = \excopatmatch{\mathtt{fst\Rightarrow 1, snd\Rightarrow 2}}
    \end{align*} %TODO: besserer name??? sodass es nicht zu lang wird
    The first example says: The function that takes a term and applies the identity function to it, does the same thing as the identity function.
    In the second example, we have a pair that contains the first value of a specific pair, and the second value of a specific pair, which is obviously just that specific pair.    
\end{example}    
    
\section{Simple Types}\label{sec:Simple Types}

We want the unification algorithm to work on typed terms. 
To start, let's look at some example terms and their types:

\begin{example}[Basic types]
    \begin{align*}
        \mathtt{False}&: \Bool\\
        \mathtt{Tup(1,True)}&: \mathsf{Pair(Int, Bool)}\\
        \excopatmatch{\mathtt{fst \Rightarrow 1, snd\Rightarrow True}}&: \mathsf{LPair(Int, Bool)}\\
        \mathtt{Cons(1, Cons(2, Nil))}&: \mathsf{List(Int)}\\
        Trues = \excopatmatch{\mathtt{hd \Rightarrow True, tl \Rightarrow} Trues}&: \mathsf{Stream(Bool)}\\
        \excopatmatch{\ap{x}\Rightarrow x.\expatmatch{\mathtt{True\Rightarrow False, False \Rightarrow True}}}&: \mathsf{Bool \rightarrow Bool}
    \end{align*}    
\end{example}
Here are the the basic types we consider in our examples:
\begin{definition}[Basic types]
    \begin{align*}
        \tau, \tau_1, \tau_2, ... ::&= \mathsf{Bool} \\
        &\partition \mathsfbrackets{Pair}{\tau_1, \tau_2}\\
        &\partition \mathsfbrackets{Lpair}{\tau_1, \tau_2}\\
        &\partition \mathsfbrackets{List}{\tau}\\
        &\partition \mathsfbrackets{Stream}{\tau}\\
        &\partition \mathsf{\tau_1 \rightarrow \tau_2}\\
        \Gamma :&= \cdot \partition x_1:\tau, \Gamma
    \end{align*}
    $\Gamma$ is the typing context, and contains distinct variables and their types.
    When we write $\Gamma = \cdot$, we mean that the typing context is empty.
\end{definition}
These of course don't cover all the types one could construct with our calculus, but they are enough to construct helpful examples.
I omitted the formal definition for natural numbers, since I use a shorthand anyways.

\subsection{Typing Rules}
To have a formal basis for how to assign the basic types to terms, we will introduce typing rules. 
When we write, $\Gamma\vdash t:\tau$, we mean that from the variables and their types in $\Gamma$, 
we can deduce that $t$ has the type $\tau$.

%% variables + booleans
\begin{minipage}{0.2\textwidth}
    \begin{prooftree}
        \AxiomC{$x:\tau \in \Gamma$}\RightLabel{\textsc{Var}}
        \UnaryInfC{$\Gamma\vdash x:\tau$}
    \end{prooftree}        
\end{minipage}
\begin{minipage}{0.35\textwidth}
    \begin{prooftree}
        \AxiomC{}\RightLabel{\textsc{True}}
        \UnaryInfC{$\Gamma\vdash\mathtt{True}:\mathsf{Bool}$}  
    \end{prooftree}
\end{minipage}
\begin{minipage}{0.35\textwidth}
    \begin{prooftree}
        \AxiomC{}\RightLabel{\textsc{False}}
        \UnaryInfC{$\Gamma\vdash\mathtt{False}:\mathsf{Bool}$}  
    \end{prooftree}
\end{minipage}

%% tuple
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t_1:\tau_1$}\RightLabel{\textsc{Tup}}
    \AxiomC{$\Gamma \vdash t_2:\tau_2 $}
    \BinaryInfC{$\Gamma \vdash \mathtt{Tup(}t_1,t_2\mathsf{)}: \mathsf{Pair(}\tau_1, \tau_2\mathsf{)}$}    
\end{prooftree}
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t: \mathsfbrackets{Pair}{\tau_1,\tau_2}$}\RightLabel{\textsc{Case-Pair}}
    \AxiomC{$\Gamma, x\vdash:\tau_1, y: \tau_2, t':\tau' $}
    \BinaryInfC{$\Gamma \vdash t.\expatmatch{\mathttbrackets{Tup}{t_1,t_2}\Rightarrow t'}:\tau' $}    
\end{prooftree}


%% nil + cons
\begin{minipage}{0.45\textwidth}
    \begin{prooftree}
        \AxiomC{}\RightLabel{\textsc{Nil}}
        \UnaryInfC{$\Gamma\vdash \Nil:\mathsf{List(\tau)}$}
    \end{prooftree}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t_1:\tau$}\RightLabel{\textsc{Cons}}
    \AxiomC{$\Gamma\vdash t_2:\mathsf{List(\tau)}$}
    \BinaryInfC{$\Gamma\vdash \mathtt{Cons(\mathnormal{t_1,t_2})}:\mathsf{List(\tau)}$}
\end{prooftree}
\end{minipage}

%% case-list
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t:\mathsf{List(\tau')}$}\RightLabel{\textsc{Case-List}}
    \AxiomC{$\Gamma\vdash t_1:\tau$}
    \AxiomC{$\Gamma, y:\tau',z:\mathsf{List(\tau')}\vdash t_2:\tau$}
    \TrinaryInfC{$\Gamma\vdash t.\expatmatch{\Nil\Rightarrow t_1, \mathtt{Cons(\mathnormal{y,z})}\Rightarrow t_2}:\tau$}
\end{prooftree}

%% stream
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t_1:\tau$}\RightLabel{\textsc{Stream}}
    \AxiomC{$\Gamma\vdash t_2:\mathsf{Stream(\tau)}$}
    \BinaryInfC{$\Gamma\vdash \excopatmatch{\mathtt{hd}\Rightarrow t_1, \mathtt{tl}\Rightarrow t_2}: \mathsf{Stream(\tau)}$}
\end{prooftree}

%% hd + tl 
\begin{minipage}{0.45\textwidth} 
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t:\mathsf{Stream(\tau)}$}\RightLabel{\textsc{Hd}}
    \UnaryInfC{$\Gamma\vdash t.\mathtt{hd}:\tau$}    
\end{prooftree}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t:\mathsf{Stream(\tau)}$}\RightLabel{\textsc{Tl}}
    \UnaryInfC{$\Gamma\vdash t.\mathtt{tl}:\mathsf{Stream(\tau)}$}
\end{prooftree}
\end{minipage}

%%lazypair
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t_1:\tau_1$}\RightLabel{\textsc{Lpair}}
    \AxiomC{$\Gamma\vdash t_2:\tau_2$}
    \BinaryInfC{$\Gamma\vdash \excopatmatch{\mathtt{fst}\Rightarrow t_1, \mathtt{snd} \Rightarrow t_2}:\mathsf{LPair(\tau_1, \tau_2)}$}
\end{prooftree}

%% fst + snd
\begin{minipage}{0.45\textwidth}
    \begin{prooftree}
        \AxiomC{$\Gamma\vdash t:\mathsf{Lpair(\tau_1,\tau_2)}$}\RightLabel{\textsc{Fst}}
        \UnaryInfC{$\Gamma\vdash t.\mathtt{fst}:\tau_1$}
    \end{prooftree}    
\end{minipage}
\begin{minipage}{0.45\textwidth}
    \begin{prooftree}
        \AxiomC{$\Gamma\vdash t:\mathsf{Lpair(\tau_1,\tau_2)}$}\RightLabel{\textsc{Snd}}
        \UnaryInfC{$\Gamma\vdash t.\mathtt{snd}:\tau_2$}
    \end{prooftree}    
\end{minipage}

%% function application and function abstraction (?)
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t_1:\tau_1\rightarrow\tau_2$}\RightLabel{\textsc{App}}
    \AxiomC{$\Gamma\vdash t_2:\tau_1$}
    \BinaryInfC{$\Gamma\vdash t_1.\ap{t_2}:\tau_2$}
\end{prooftree}
\begin{prooftree}
    \AxiomC{$\Gamma,x: \tau_1\vdash t:\tau_2$}\RightLabel{\textsc{Fun}}
    \UnaryInfC{$\Gamma\vdash\excopatmatch{\ap{x}\Rightarrow t}:\tau_1\rightarrow \tau_2$}
\end{prooftree}    


\section{First-Order Unification}\label{sec:First-Order Unification}

We want to slowly make our way to higher-order unification, and thus touch on simpler problems first. 
Furthermore, we need a couple more concepts to talk about unification problems and describe our algorithm. 

A unification problem is described by a set of equations with expressions on each side $\overline{e\equiv e}$ containing unknown unification variables $\alphaunifvar_1, \alphaunifvar_2, \betaunifvar...$, 
where our goal is to find a simultaneous substitution $[\listofexpr / \alphaunifvar_1, \dots, \alphaunifvar_n]$ which substitutes expressions for unification variables, 
such that the sides of the given equations are the same. 

\begin{definition}[Solution]
    A solution to a given unification problem is described by a simultaneous substitution $[\listofexpr / \alphaunifvar_1, \dots, \alphaunifvar_n]$
    which when applied to the problem solves it, i.e. makes the sides of the equations equal.
\end{definition}
    
\begin{definition}[Most General]
    A solution is the most general unifier (mgu), iff. it is more general (see \cref{more general}) than all other solutions.
\end{definition} 

Now let's take a look at first-order unification.
Even though first-order unification is a pretty limited subproblem of higher-order unification, 
in many applications it is all that is needed --- the types of most (non-dependent) programming languages can be represented with first-order terms.

Consider some examples of first-order unification. On the left is the problem, on the right its solution or $\bot$ if there is no solution.
\begin{example}[First-order unification]
    \begin{align*}
        \alphaunifvar &\equiv \True &\sigma_1 &= [\True/\alphaunifvar]\tag{1}\\
        \True &\equiv \False &\sigma_2 &= \bot\tag{2}\\
        \Int \rightarrow \alphaunifvar &\equiv \Int \rightarrow \Bool &\sigma_3 &= [\Bool/\alphaunifvar]\tag{3}\\
        \alphaunifvar&\equiv \mathsf{List(\alphaunifvar)} &\sigma_4 &= \bot\tag{4}
    \end{align*}
    The forth example is an instance of a failing \ref{a}.
\end{example}

\begin{definition}[First-order unification]
    A first-order unification problem consists of expressions from this restricted grammar:
\begin{align*}
    e,r,s,t ::&= \alphaunifvar, \betaunifvar \tag*{Unification variable}\\
    &\partition x\tag*{Variable} \\
    &\partition \constructor \tag*{Constructor}
\end{align*}
\end{definition}

\begin{theorem}[Decidability of First-Order Unification]
    For first-order unification, there exists an algorithm on equations $\overline{e\equiv e}$, which always terminates, and returns the solution if there exists one. 
    In particular, this solution is always a mgu (i.e. if there is a solution, then there always exists a most general one).
\end{theorem}%TODO: citation

\begin{definition}[Unification algorithm for First-Order Unification ]
    $\perp$ is the symbol for fail.
    The algorithm is defined by non-deterministically applying the rules below:
    \begin{align*}
        E & \cup \{ e \equiv e\} \Rightarrow E \tag{delete}\\
        E & \cup \{\exconstructor{e_1, ..., e_n} \equiv \exconstructor{t_1, ..., t_n}\} \Rightarrow E \cup \{e_1 \equiv t_1, ..., e_n \equiv t_n\}\tag{decompose}\\
        E & \cup \{K_1(e_1,..., e_n) \equiv K_2(t_1, ..., t_m)\}  \Rightarrow \perp \quad\text{if $K_1 \neq K_2$ or if $n\neq m$} \tag{conflict}\\
        E & \cup \{e \equiv \alphaunifvar\} \Rightarrow E \cup \{\alphaunifvar \equiv e\}\tag{swap}\\ 
        E & \cup \{\alphaunifvar \equiv e\} \Rightarrow E[e/\alphaunifvar] \cup \{\alphaunifvar \equiv e\} \quad\text{if $\alphaunifvar \in E$ and $\alphaunifvar \notin e$}  \tag{eliminate}\\
        E & \cup \{\alphaunifvar \equiv e\}\Rightarrow \perp \quad \text{if $\alphaunifvar \in e$} \tag{occurs check} \label{a}
    \end{align*}
\end{definition}

This algorithm is based on the version presented by Martelli and Montanari in \cite{10.1145/357162.357169},
adapted to our syntax.

\section{Higher-Order Unification}\label{sec:Higher-Order Unification}

In this section, I introduce higher-order unification.

\begin{definition}[Higher-Order Unification]
    A higher-order unification problem consists of expressions from the following grammar:
    \begin{align*}
        e,r,s,t  ::&= \alphaunifvar\sigma \tag*{Unification variable with substitution}\\ %TODO: name? 
            &\partition x  \tag*{Variable} \\
            &\partition\constructor \tag*{Constructor} \\
            &\partition\patmatch  \tag*{Pattern match}\\
            &\partition\copatmatch  \tag*{Copattern match}\\
            &\partition\destructor  \tag*{Destructor}
    \end{align*}
\end{definition}

Note that this is encompassed by our syntax described in \cref{sec:syntax},
but with the addition of unification variables with substitutions. 
To illustrate the need for this substitution, look at what problem arises when we omit the substitution:
\begin{example}[Substitutions on unification variables] 
    Consider this example:
    \begin{align*}
        \excopatmatch{\ap{x}\Rightarrow\alphaunifvar}.\ap{y}\equiv\betaunifvar
    \end{align*}
    If you focus on the left side, you might notice that there is a redex. What happens if we reduce it?
    \begin{align*}
        \excopatmatch{\ap{x}\Rightarrow\alphaunifvar}.\ap{y}\betaconv\alphaunifvar[y/x]    
    \end{align*}     
    If we now found the solution $\sigma = [x/ \alphaunifvar]$ through another equation, we would actually need to substitute $y$ for $x$ in $\alphaunifvar = x$!
\end{example}

This motivates our need for substitutions on unification variables: 
Since we don't know what the solution for a unification variable will be, we might need to perform a substitution on it later.
Note that this is \textit{not} possible in first-order unification, since we don't create substitutions through redexes! 
When the substitution is trivial, we may  write $\alphaunifvar, \betaunifvar$ instead.

In higher-order unification in contrast to first-order unification, we are not interested in syntactic equality, but want a broader set of terms to be equal to one another.
Depending on the type of unification problem one wants to solve, one may want to only include beta-equality or both beta- and eta-equality.
This further motivates the use of the symbol $\equiv$ so far. Whereas in first-order unification it just stands for syntactic equality, 
in higher-order unification, I use it to mean syntactic equality, beta-equality or eta-equality.
This essentially means that two terms are equivalent if they are equivalent after function application, (co-)pattern matching evaluation, and/or are equivalent extensionally.

In the introduction in \cref{sec:mgu}, I alluded to the fact that many problems have multiple solutions, but we prefer a certain, most general solution.
Let's next consider a familiar example again:
\begin{example}[No most general solution]
    \begin{align*}
        \alphaunifvar.\ap{5} \equiv \texttt{5}
    \end{align*}
    This problem has multiple solutions:
    \begin{align*}
        \sigma_1 = [\excopatmatch{\ap{x}\Rightarrow x} / \alphaunifvar]\tag*{identity function}\\
        \sigma_2 = [\excopatmatch{\ap{x}\Rightarrow 5} / \alphaunifvar]\tag*{constant function, always returns 5}
    \end{align*}
    In this case, neither solution is more general. The reason for this becomes apparent when you take a look at the action of a substitution again. 
Let's say we wanted to find a substitution to apply after the identity function to get the constant function. 
When we apply a substitution to a copattern match, we substitute new variables for our bound variables to not accidentally change the meaning of the term.
This prevents us from changing our function. 
Since neither solution is more general (and there also is no other solution that is more general),
there is no mgu for this problem!
\end{example}

Unification problems having no mgu is specific to higher-order unification, since in first-order unification we don't look at 
higher-order terms and thus don't have that problem of not changing variables.

\begin{theorem}[Decidability of higher-order unification]
    Higher-order unification is not decidable. This can be proven through reducing Hilbert's tenth problem to the unification problem.%TODO: schmöker citen? oder wird das da auch nicht wirklich gemacht?
\end{theorem} %TODO: citation

\subsection{Pattern Unification}

Pattern unification is a helpful tool for certain terms called \textbf{patterns}, which were first described by Miller in \cite{10.1093/logcom/1.4.497}.
While Miller described patterns for the lambda calculus, we need to extend that definition, since our calculus ND is also an extension of the lambda calculus. 

\begin{definition}[Pattern]
    A pattern is any term $p$ 
    \begin{align*}
        p ::= \alphaunifvar \partition p.d(\listofvar)
    \end{align*}
    where it holds that all $\listofvar$ are distinct variables.
\end{definition}

Consider some examples for patterns. On the left is the pattern unification problem and on the right its solution:
\begin{example}[Pattern]
    \begin{align*}
        \alphaunifvar.\ap{x}.\ap{y}&\equiv x &\sigma_1 &= [\excopatmatch{\ap{x}\Rightarrow\excopatmatch{\ap{y}\Rightarrow x}}/\alphaunifvar] \tag{1}\\
        \alphaunifvar.\mathtt{fst} &\equiv 2 &\sigma_2 &= [\excopatmatch{\mathtt{fst\Rightarrow 2, snd \Rightarrow}\betaunifvar}/\alphaunifvar] \tag{2}
    \end{align*}   
\end{example}

Note that even first-order unification contains terms that are not patterns, i.e.
patterns are not a subset of first-order unification, only of higher-order unification.

\begin{theorem}[Decidability of Pattern Unification]
    Unification on patterns is decidable.
    Iff. there exists a solution, there also exists a mgu.
\end{theorem}
Thus, the nice thing about pattern unification is that if we restrict our higher-order unification problem to only contain unification variables in patterns, 
finding a solution becomes much easier. 

The reason there always exists an mgu lies in the constraint we put on our definition: All the variables must be distinct. 
To illustrate this, take the following problem where the variables are \textbf{not} distinct:
\begin{example}[Why distinct variables in patterns?]
    \begin{align*}
        \alphaunifvar.\ap{x}.\ap{x}\equiv x
    \end{align*}  
    We can name two solutions:
\begin{align*}
    \sigma_1 &= [\excopatmatch{\ap{x}\Rightarrow \excopatmatch{\ap{y}\Rightarrow x}}/\alphaunifvar]\\
    \sigma_2 &=[\excopatmatch{\ap{x}\Rightarrow \excopatmatch{\ap{y}\Rightarrow y}}/\alphaunifvar]  
\end{align*}
(Intuitively, the solutions say to select the first or second argument of the function applications, respectively.)
These solutions are equivalent, in that no solution is more general than the other. Thus, there exists no mgu for this problem.
The solutions aren't unique because the variables aren't unique.  
\end{example}

%TODO: boring
To talk about the algorithms for solving unification problems, we need another definition: %TODO: umformulieren
\begin{definition}[Normal Form]
    The normal form \textbf{NF} is defined as follows:
    \begin{align*}
        n &::= x \partition \alphaunifvar \partition n.d(\overline{v})\partition n.\textbf{case}\{\overline{K(\overline{x})\Rightarrow v}\}\\
        v &::= n \partition K(\overline{v}) \partition \excopatmatch{\overline{d(\overline{x})\Rightarrow v}}
    \end{align*}
    Terms that satisfy the $n$-definition are called neutral terms, $v$-terms are called values.
\end{definition}
Note that these are the terms that do not contain beta-redexes (a term that can be reduced through beta-conversion).
This is apparent when one tries to construct terms that do beta-redexes in normal form: 
To construct the first kind of beta-redex: $\constructor.\textbf{case}\{\dots,\exconstructor{\overline{x}}\Rightarrow e\}$,
we start with the term $n.\textbf{case}\{\overline{K(\overline{x})\Rightarrow v}\}$, and now want to substitute $K(\overline{e})$ for $n$. 
This is not possible because we are limited to neutral terms, which constructors are not a part of.
Similarly for the second kind of beta-redex:
$\excopatmatch{\dots,d(\overline{x}) \Rightarrow e, \dots}.d(\overline{e})$, 
here we can not substitute the cocase in $n.d(\overline{e})$ because we are limited to neutral terms.
        
This underlines the intuition that neutral terms are terms that cannot be reduced because
they contain a term which blocks the evaluation (either a variable or a unification variable) in the front.
Even though we don't have this assurance in values, that is not a problem since values don't contain the building blocks for beta-reductions. 
Some examples of terms in normal form: 
\begin{example}[Normal form]
\begin{align*}
    \alphaunifvar.\ap{\excopatmatch{\ap{x}\Rightarrow x}} \tag{1}\\
    \excopatmatch{\ap{x}\Rightarrow \mathtt{Cons(\mathnormal{x}, Cons(\mathnormal{y}, Nil))}} \tag{2}\\
    \alphaunifvar.\ap{x_1, x, y, z} \tag{3}    
\end{align*}
The first and second example are in normal form and the third is a pattern in normal form.
\end{example}

\begin{theorem}
    A unification problem has a solution if and only if that solution is the same for the normal form of that problem. 
\end{theorem}

This is helpful for us since we only have to consider normal forms in our algorithm.
Concretely: We first reduce a given term it to its normal form, and then apply the steps to find our solution.
Thus, from this point on, we will only be looking at terms that have a normal form. This is enough in most applications. %TODO: ausschmücken?

For a practical example for why normal forms are helpful, consider the following problem:
\begin{example}[Beta-reductions before solving]
    \begin{align*}
        \True.\expatmatch{\True \Rightarrow \alphaunifvar, \False \Rightarrow \mathtt{2}} \equiv \mathtt{3} \tag{1}
    \end{align*}
    has the solution $[\mathtt{3/\alphaunifvar}]$, but this is more obvious after performing a beta-reduction on the left side 
    to bring it into normal form:
    \begin{align*}
        &\True.\expatmatch{\True \Rightarrow \alphaunifvar, \False \Rightarrow \mathtt{2}} \betaconv \alphaunifvar\\
        &\implies \alphaunifvar \equiv \mathtt{3}   
    \end{align*}       
Let's take a look at another unification problem where beta-conversion is helpful:
\begin{align*}
    &\alphaunifvar \equiv \True \tag{2}\\
    &\alphaunifvar.\expatmatch{\mathtt{True\Rightarrow 2, False\Rightarrow 3}} \equiv \betaunifvar
\end{align*}
To find out that $[\mathtt{2}/\betaunifvar]$ is the solution to the second equation, we first need to find out that
$[\True/\alphaunifvar]$ is the solution to the first equation and substitute it in the second.
This is what is called \textbf{dynamic unification}, where we hold off on solving some equations 
until we have all the necessary information. 
 
\end{example}

\section{The Algorithm For Higher-Order Unification}\label{The Algorithm For Higher-Order Unification}
In this section, I present the algorithm for higher-order unification.
First, we need a framework to solve our algorithm in.
\subsection{Constraints}

Even though higher-order unification is undecidable, in many cases we can still find solutions,
or conclude that there is no solution.
So far, we have only described the problem of higher-order unification. Now we want to detail the steps necessary to solve higher-order unification problems.
Thus, we need a framework for our equations --- one where we can simplify our equations,
and store what we have found out about our unknown variables. This is where constraints come into play:

\begin{definition}[Constraint]
    \begin{align*}
        C ::= \top \partition \bot \partition \Psi \vdash e\equiv t \tag*{where $\Psi = x_1, ..., x_n$}\\
        \mathcal{C} ::= C \wedge \mathcal{C}
    \end{align*}
    Where $\top$ means that an equation is trivially true, whereas $\bot$ is for contradictory equations. 
    The variables in the context $\Psi$ need to be distinct. $\Psi$ may also be empty.
\end{definition}
$\Psi \vdash e\equiv t$ means that given the variables $\Psi$, we deduce that $e\equiv t$.
$\Psi$ contains those variables we have seen before and want to remember.
More precisely, $\Psi \vdash e\equiv t \Rightarrow FV(e)\subseteq \Psi \wedge FV(t)\subseteq \Psi$.
This motivates the name context for $\Psi$, as well.

We formulate our constraints from a given unification problem as follows:
We take each given equation and formulate a constraint $K$ with empty context $\Psi$. We join them using ands:
The equations $\overline{e\equiv t}$ become $\constraints = C_1 \wedge ... \wedge C_n = \, \vdash e_1 \equiv t_1 \wedge ...\wedge \vdash e_n \equiv t_n $  

For clarification, $C \wedge \mathcal{C}$ is the regular logical "and", so therefore 
$\top \wedge \constraints$ is equivalent to $\constraints$ as well as $\bot \wedge \constraints$ to $\bot$.
We can also apply substitutions to a set of constraints, which just means applying the substitution to the equation part of all the constraints in the set:
$\constraints \sigma = (C \wedge \constraints')\sigma = C \sigma \wedge \constraints' \sigma  
= \Psi \vdash e\sigma\equiv t\sigma\wedge \constraints'\sigma$. Applying a substitution to $\top$ or $\bot$ changes nothing.

\subsection{Decomposing Constraints}

It is helpful to simplify our equations a bit before starting the unification process.
We do this by taking out redundant equations, spotting clearly contradictory equations or splitting our equations into smaller parts which we can further simplify and unify.

Let's look at some examples. We would like the following to be true:
\begin{example}[There are two kinds of variables!]
    \begin{align*}
    &x \vdash x.\mathtt{fst} \equiv x.\mathtt{fst} &\mapsto& \top
    \tag{1}\\
    &x \vdash x.\mathtt{fst} \equiv x.\mathtt{snd} &\mapsto& \bot 
    \tag{2}\\
    &x,y \vdash x.\mathtt{fst} \equiv y.\mathtt{fst} &\mapsto& \bot 
    \tag{3}
    \end{align*}
    These examples might be confusing at first. Looking at the second example, one might argue that there exists a pair, say $x = \excopatmatch{\mathtt{fst\Rightarrow 2, snd\Rightarrow 2}}$, such that
    $x.\mathtt{fst} = x.\mathtt{snd}$. This is where it is important to remember that we are not looking for solutions to variables like $x$.
    Whereas to our unification variables $\alphaunifvar$ we want to assign any term such that our equations hold, variables $x$ add \textbf{constraints} to our equations. 
    The equation $x \vdash x.\mathtt{fst} \equiv x.\mathtt{snd}$ must hold for \textbf{any} variable $x$, since we cannot choose $x$! 
    Since for $x = \excopatmatch{\mathtt{fst\Rightarrow 2, snd\Rightarrow 3}}$, the equation does not hold, we can simplify it to $\bot$.
    %TODO: rework sentence structure
    %TODO: can i explain this better?
    %TODO: write something about the third example?
\end{example}

\begin{example}[Constructors and Destructors]
    \begin{align*}
    &\vdash \mathttbrackets{Cons}{x, \mathtt{Nil}} \equiv \mathttbrackets{Cons}{y, \mathtt{Nil}}&\mapsto& \bot
    \tag{1}\\
    &\vdash \texttt{Cons(}\alphaunifvar, \texttt{Nil)}\equiv\texttt{Cons(}\betaunifvar, \texttt{Nil)}
    &\mapsto&\vdash \alphaunifvar\equiv \betaunifvar
    \tag{2}\\
    &\vdash \mathsf{List(Int)}\equiv \mathsf{List(\alphaunifvar)}
    &\mapsto& \vdash \mathsf{Int}\equiv \alphaunifvar
    \tag{3}\\
    &x \vdash x.\ap{e_1}\equiv x.\ap{e_2} &\mapsto& x\vdash e_1\equiv e_2 
    \tag{4}
    \end{align*}
\end{example}

\begin{example}[Pattern and Copattern Matching]
    \begin{align*}
    &x\vdash x.\expatmatch{\texttt{T} \Rightarrow e_1, \texttt{F} \Rightarrow e_2} \equiv x.\expatmatch{\texttt{T} \Rightarrow t_1, \texttt{F} \Rightarrow t_2}\footnotemark[1]\tag{1}
    \\ &\mapsto x\vdash  e_1 \equiv t_1\wedge x\vdash e_2\equiv t_2\\
    &\vdash \excopatmatch{\ap{x}\Rightarrow \alphaunifvar} \equiv \excopatmatch{\ap{x}\Rightarrow\betaunifvar} \tag{2}
    \\ &\mapsto x\vdash \alphaunifvar \equiv \betaunifvar\\
    &\excopatmatch{\mathtt{fst \Rightarrow} \mathtt{5}, \mathtt{snd\Rightarrow False}}\equiv \alphaunifvar \tag{3}
    \\ &\mapsto \vdash \mathtt{5}\equiv \alphaunifvar.\mathtt{fst}\, \wedge \,\vdash \mathtt{False\equiv\alphaunifvar.snd}
    \end{align*}
\end{example}
\footnotetext[1]{\texttt{T} and \texttt{F} are abbreviations for $\True$ and $\False$.}
Note that all of these examples are in normal form, i.e. they don't contain redexes.
This means the only way for the terms on each side of the equations to be equivalent,
is for them to be equivalent \textbf{syntactically}. 

We want to formulate rules which help us simplify equations like these --- equations without beta-redexes.
We want remove redundant constraints, split constraints made up of composite terms to find solutions to their parts,
and spot redundancies to stop the solving process as early as possible
\begin{definition}[Decomposing Constraints]
    \begin{align*}
    &\textbf{Removing redundancy}\\
    &\Psi \vdash x\equiv x&\mapsto_r&\top
    \tag{1}\label{1}\\
    &\Psi \vdash \alphaunifvar \equiv \alphaunifvar&\mapsto_r& \top
    \tag{2}\\
    &\textbf{Decomposition}\\
    \circ\,&\Psi\vdash K(\overline{e})\equiv K(\overline{t})&
    \mapsto_d&\overline{\Psi\vdash e\equiv t}
    \tag{3}\label{3}\\
    \circ\,&\Psi \vdash e_1.d({\overline{e}})\equiv e_2.d({\overline{t}})\tag{4}\label{4}
    \\ &\mapsto_d \Psi\vdash e_1 \equiv e_2 \wedge \overline{\Psi\vdash e\equiv t}\\
    *\,&\Psi \vdash e_1.\expatmatch{\overline{K(\overline{x})\Rightarrow e}}\equiv e_2.\expatmatch{\overline{K(\overline{x})\Rightarrow t}}\tag{5}
    \\ &\mapsto_d \Psi \vdash e_1 \equiv e_2\wedge \overline{\Psi, \overline{x}\vdash e\equiv t}\\
    *\, &\Psi\vdash \excopatmatch{\overline{d(\overline{x})\Rightarrow e}}\equiv 
    \excopatmatch{\overline{d(\overline{x})\Rightarrow t}}
    &\mapsto_d& \overline{\Psi, \overline{x}\vdash e \equiv t}
    \tag{6}\label{6}\\
    &\Psi\vdash \excopatmatch{\overline{d(\overline{x})\Rightarrow e}}\equiv t 
    &\mapsto_d& \overline{\Psi,\overline{x} \vdash e \equiv t.d(\overline{x})}
    \tag{7}\label{7}\\ 
    &\Psi\vdash t\equiv \excopatmatch{\overline{d(\overline{x})\Rightarrow e}} 
    &\mapsto_d& \overline{\Psi, \overline{x}\vdash t.d(\overline{x}) \equiv e}
    \tag{8}\label{8}\\
    &\textbf{Eta-reduction}\\
    & \Psi \vdash \excopatmatch{\overline{d(\overline{x})\Rightarrow e.d(\overline{x})}}\equiv t
    &\mapsto_e&\Psi\vdash e\equiv t
    \tag{9}\\
    &\textbf{Removing contradictions}\\ 
    &\Psi \vdash x\equiv y &\mapsto_c& \bot 
    \tag{10}\\
    &\text{non-matching con-/destructors or non-matching}\\
    &\text{variables in }\text{$\circ$-equation} &\mapsto_c& \bot\\
    &\text{non-matching con-/destructors in *-equation}&\mapsto_c&\bot
    \tag{12}\label{12}\\
    \end{align*}
\end{definition}
Rules 6-8 are taken from \cite{10.5555/2021953.2021960}, adapted to our syntax. 

For equations marked with *, we require the constructors (or destructors) to be equal to one another in each equation. 
We also require them to have the same list of variables as arguments, respectively (i.e. full syntactic equality).
For equations marked with $\circ$, we only require the constructors (or destructors) to be equal to one another in each equation. 
(i.e. no syntactic equality among arguments required).
This is because in *-equations, we expect variables, whereas in $\circ$-arguments, we expect expressions (or \textbf{values} to be exact --- since we expect terms in normal form).

In *-equations where the arguments are not equal syntactically, rule \cref{12} applies and we simplify to $\bot$:

\begin{example}[Contradiction in (co-)pattern matching]
    \begin{align}
        &\vdash x.\expatmatch{\mathtt{True\Rightarrow \alphaunifvar, False \Rightarrow \betaunifvar}} 
        \equiv x.\expatmatch{\mathtt{1\Rightarrow \alphaunifvar, 2\Rightarrow\betaunifvar}}
        &\mapsto_c& \bot \tag{1}\\
        &\vdash \excopatmatch{\mathtt{fst\Rightarrow 1, snd\Rightarrow 2}}
        \equiv \excopatmatch{\ap{x}\Rightarrow \mathtt{1}}
        &\mapsto_c& \bot \tag{2}
    \end{align}
\end{example}

Also note that \cref{7} (as well as \cref{8} wich is just the mirror of \cref{7}) are possible through eta-conversion:
\begin{align*}
    \excopatmatch{\overline{d(\overline{x})\Rightarrow e}}&\equiv t \\
    \excopatmatch{\overline{d(\overline{x})\Rightarrow e}}&\equiv \excopatmatch{\overline{d(\overline{x})\Rightarrow t.d(\overline{x})}}\\
    &\xmapsto{(\cref{6})}_d \overline{e \equiv t.d(\overline{x})}
\end{align*}

\subsection{Unification}

What do we do when we can't simplify anymore? How do we actually find the solutions?
Let's start with an example --- take a look at the following unification problem:
\begin{example}[Unification]
    \begin{align*}
        \mathtt{Tup(\alphaunifvar, False)}&\equiv \mathtt{Tup(True, False)}\\
        \betaunifvar&\equiv \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}
    \end{align*}        
    I will demonstrate how we want to solve this, and in the process introduces what to do when we found a solution.   
    \begin{align*}
        \constraints &= &\vdash\ & \mathtt{Tup(\alphaunifvar, F)}\equiv \mathtt{Tup(T, F)}&\wedge&
        \vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{T \Rightarrow 1, F\Rightarrow 5}}\footnotemark[1]\\
        &\xmapsto{(\cref{3})}_d &\vdash\ & \mathtt{\alphaunifvar\equiv T}\qquad\wedge\vdash \mathtt{F\equiv F} &\wedge& 
        \vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{T \Rightarrow 1, F\Rightarrow 5}}\\
        &\xmapsto{(\cref{3})}_d &\vdash\ & \mathtt{\alphaunifvar\equiv T} &\wedge& 
        \vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{T \Rightarrow 1, F\Rightarrow 5}}\\
        &\mapsto_u &\vdash\ &\mathtt{\alphaunifvar\equiv T} &\wedge&
        \vdash \betaunifvar\equiv \mathtt{T}.\expatmatch{\mathtt{T \Rightarrow 1, F\Rightarrow 5}}\\
        &\stackrel{\betaconv}{=} &\vdash\ &\mathtt{\alphaunifvar\equiv T} &\wedge&
        \vdash \betaunifvar\equiv \mathtt{1}
    \end{align*}
What do we learn about unification steps from this example? When we have found an assignment to a unification variable, we want to keep it in our constraints!
This is because we sometimes have to substitute that assignment in other constraints that contain the same unification variable.
But this example tells us something else, too: Doing so might introduce redexes!
This means that we might need to reduce terms to normal form again, even after applying unification steps. 
\end{example}

\iffalse
\begin{align*} %overfull
    \constraints &= &\vdash\ & \mathtt{Tup(\alphaunifvar, False)}\equiv \mathtt{Tup(True, False)}&\wedge&
    \vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\xmapsto{(5)}_d &\vdash\ & \mathtt{\alphaunifvar\equiv True}\qquad\wedge\vdash \mathtt{False\equiv False} &\wedge& 
    \vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\xmapsto{(3)}_r &\vdash\ & \mathtt{\alphaunifvar\equiv True} &\wedge& 
    \vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\mapsto_u &\vdash\ &\mathtt{\alphaunifvar\equiv True} &\wedge&
    \vdash \betaunifvar\equiv \mathtt{True}.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\stackrel{\betaconv}{=} &\vdash\ &\mathtt{\alphaunifvar\equiv True} &\wedge&
    \vdash \betaunifvar\equiv \mathtt{1}
\end{align*} 
\begin{align*} %split lines
    \constraints &= \vdash\ \mathtt{Tup(\alphaunifvar, False)}\equiv \mathtt{Tup(True, False)}\\
    &\wedge\vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\xmapsto{(5)}_d \vdash\   \mathtt{\alphaunifvar\equiv True}\qquad\wedge\vdash \mathtt{False\equiv False}\\
    &\wedge\vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\xmapsto{(3)}_r \vdash\   \mathtt{\alphaunifvar\equiv True} \\
    &\wedge\vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\mapsto_u \vdash\  \mathtt{\alphaunifvar\equiv True}\\
    &\wedge\vdash \betaunifvar\equiv \mathtt{True}.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\stackrel{\betaconv}{=} \vdash\  \mathtt{\alphaunifvar\equiv True} \\
    &\wedge\vdash \betaunifvar\equiv \mathtt{1}
\end{align*}
\fi

There are very few unification steps: 
\begin{definition}[Unification]
    \begin{align*}
        & e \equiv\alphaunifvar
        &\mapsto_u& \alphaunifvar \equiv e &(e\neq \betaunifvar)
        \tag{1}\\
        &\constraints \wedge \Psi \vdash \alphaunifvar \equiv e 
        &\mapsto_u& \constraints[e/\alphaunifvar] \wedge \Psi \vdash \alphaunifvar \equiv e &(\alphaunifvar \notin e)
        \tag{2}\label{unif2}\\
        &\constraints \wedge \Psi \vdash \alphaunifvar \equiv e
        &\mapsto_u& \bot &(\alphaunifvar\in e)
        \tag{3}
    \end{align*}
Step 2 and 3 are the occurs check, which ensures that we don't infinitely substitute terms for unification variables.
\end{definition}

We should be careful, because there is another thing we could introduce with unification steps:
\begin{example}[Introducing simplifiable constraints through unficiation steps]
    \begin{align*}
        \constraints &=& &\vdash \excopatmatch{\ap{x}\Rightarrow\alphaunifvar}\equiv id 
        &\wedge& &\vdash \betaunifvar.\ap{x}\equiv \alphaunifvar\\
        &\xmapsto{(\ref{7})}_d& &x \vdash \alphaunifvar \equiv id.\ap{x}
        &\wedge& &\vdash \betaunifvar.\ap{x}\equiv \alphaunifvar\\
        &\xmapsto{(\ref{unif2})}_u& &x \vdash \alphaunifvar \equiv id.\ap{x}
        &\wedge& &\vdash \betaunifvar.\ap{x}\equiv id.\ap{x}\\
        &\xmapsto{(\ref{4})}_d& &x \vdash \alphaunifvar \equiv id.\ap{x} 
        &\wedge& &\vdash \betaunifvar \equiv id \wedge \vdash x \equiv x\\
        &\xmapsto{(\ref{1})}_r& &x \vdash \alphaunifvar \equiv id.\ap{x}
        &\wedge& &\vdash \betaunifvar \equiv id
    \end{align*}
    Here, the unification step introduces another constraint we can simplify! 
\end{example}

\subsection{The algorithm}
Having described all the steps of our algorithm, the given examples lead us to a question:
How and when do we apply these steps?

Given a set of equations, we normalize the terms in them and are left with only normal forms. 
For each equation, we formulate a constraint and get the set of constraints through logical ands.
Next, we decompose the constraints, removing redundancies and potentially terminating early since we find contradictions.
We do this non-deterministically, since the order doesn't matter. %TODO: does it really not?
This makes our algorithm dynamic, since we hold off on solving certain equations until we know more.

When we can't further simplify, we non-deterministically apply unification steps, beta-reduce our terms and apply decomposition steps.
When we have found an assigned term for each unification variable, we can terminate and give the solution
by formulating a substitution for each assigned term.

This algorithm works on any pattern unification problem, and in particular, any unification problem that can be reduced to a pattern unification problem. %TODO: how?

\section{Related works}\label{sec:Related works}

%codata zuerst beschrieben? https://www.sciencedirect.com/science/article/pii/S0747717189800653
In \cite{HAGINO1989629}, the author presents codata types in the programming language ML

%codata in action
Codata in action \cite{10.1007/978-3-030-17184-1_5} was an important motivating force to research codata more.

The calculus ND is taken from \cite{binder2024programming}.

%das erste mal wo unifikation beschrieben wurde?
%der unifikationsalgorithmus für first order unif (martelli)
Martelli and Montanari presented the algorithm for first-order unification problems
which is often used today. \cite{10.1145/357162.357169}
%pattern unification
Pattern unfication is first described in \cite{10.1093/logcom/1.4.497}.
%higher-order unification schmöker

%abel+pientka für codata (actually dependent types)
%   -> evt hervorheben dass die super viele sachen dazu machen und dass der eine agda oder so? macht
In \cite{10.5555/2021953.2021960}, the authors describe a higher-order unification algorithm for dependent types and records.



%davids phd für den syntax


\section{Conclusion}\label{sec:conclusion}
In summary, I presented a higher-order unification algorithm solving unification problems containing patterns 
for languages with a strong data-codata duality.
This is an important improvement on previously presented higher-order unification algorithms
which don't take codata types apart from (dependent) function types into account. 
The higher-order unification algorithm presented is a step towards 
being able to use languages with strong dualities like the data-codata duality in practice,
as it shows that type-checking and -verification is very much possible.
I hope that this as well as the exploration of the syntax make others more inclined to consider and use codata.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\addcontentsline{toc}{chapter}{Bibliography}

\bibliographystyle{abbrv}
\bibliography{mylit}
%% Obige Anweisung legt fest, dass BibTeX-Datei `mylit.bib' verwendet
%% wird. Hier koennen mehrere Dateinamen mit Kommata getrennt aufgelistet
%% werden.

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Erklaerung
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}
\section*{Selbständigkeitserklärung}

Hiermit versichere ich, dass ich die vorliegende Bachelorarbeit 
selbständig und nur mit den angegebenen Hilfsmitteln angefertigt habe und dass alle Stellen, die dem Wortlaut oder dem 
Sinne nach anderen Werken entnommen sind, durch Angaben von Quellen als 
Entlehnung kenntlich gemacht worden sind. 
Diese Bachelorarbeit wurde in gleicher oder ähnlicher Form in keinem anderen 
Studiengang als Prüfungsleistung vorgelegt. 

\vskip 3cm

Ort, Datum	\hfill Unterschrift \hfill 


%%% Ende
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

\iffalse
Für wenn wir patterns in mehr kontext behandeln i guess

Some examples of patterns:
\begin{align*}
    \begin{split}
    t_1 &= \excopatmatch{\ap{x}\Rightarrow \alphaunifvar.\ap{x}} \\
    t_2 &= \excopatmatch{\ap{x}\Rightarrow \excopatmatch{\ap{y}\Rightarrow (\alphaunifvar.\ap{y}).\ap{x}}}    
    \end{split}
\end{align*} 
In each of these terms, our unification variable is applied to distinct variables which are bound through  cocases.

However, these terms are not patterns:
\begin{align*}
    \begin{split}
        t_3 &= (\alphaunifvar.\ap{x}).\ap{y}\\
        t_4 &= \excopatmatch{\ap{y}\Rightarrow (\alphaunifvar.\ap{x})}\\
        t_5 &= \excopatmatch{\ap{x}\Rightarrow (\alphaunifvar.\ap{x}).\ap{x}}
    \end{split}
\end{align*}
as they all don't contain exactly one cocase per variable which would bind that variable distinctly.
%TODO: macht das sinn mit dem distinctly? 
\fi
