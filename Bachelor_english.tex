%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% LaTeX-Rahmen fuer das Erstellen von englischen Bachelorarbeiten
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% allgemeine Einstellungen
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[twoside,12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
%\usepackage{reportpage}
\usepackage{epsf}
\usepackage{graphics, graphicx}
\usepackage{latexsym}
\usepackage[margin=10pt,font=small,labelfont=bf]{caption}

%\usepackage{fontspec}
%\setmainfont{Asana-Math}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{array} %for the \newcolumntype macro
\usepackage{amsthm} %for theorems/definitions
\usepackage{thmtools} %for listofheorems TODO: remove/integrate?
\usepackage{bussproofs}

\textwidth 14cm
\textheight 22cm
\topmargin 0.0cm
\evensidemargin 1cm
\oddsidemargin 1cm
%\footskip 2cm
\parskip0.5explus0.1exminus0.1ex

% Kann von Student auch nach persönlichem Geschmack verändert werden.
\pagestyle{headings}

\usepackage[symbol]{footmisc}
\usepackage{hyperref}

\sloppy

\begin{document}


%%%%% Macros %TODO: sollten die wo anders hin/in ein eigenes file?


%Macros for Theorems/Definitions/...
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{example}{Example}[section]

%Macros for possible terms in ND:
\newcommand{\constructor}{K(\overline{e})}
\newcommand{\destructor}{e.d(\overline{e}) }
\newcommand{\patmatch}{e.\textbf{case}\{\overline{K(\overline{x})\Rightarrow t}\}} 
\newcommand{\copatmatch}{\textbf{cocase}\{\overline{d(\overline{x})\Rightarrow t}\}}

%Macros for terms but for examples and such
\newcommand{\exconstructor}[1]{K(#1)}
\newcommand{\exdestructor}[1]{e.d(#1) }
\newcommand{\expatmatch}[1]{\textbf{case}\{#1\}} 
\newcommand{\excopatmatch}[1]{\textbf{cocase}\{#1\}}

%Macros for structural/technical latex stuff
\newcolumntype{L}{>{$}l<{$}} % a left-aligned column in mathmode
\newcolumntype{C}{>{$}c<{$}} % a center-aligned column in mathmode

\newcommand{\alphaunifvar}{\alpha^?}
\newcommand{\betaunifvar}{\beta^?}
\newcommand{\FV}[1]{\text{FV}(#1)}
\newcommand{\listofexpr}{e_1,...e_n}
\newcommand{\listofvar}{x_1,...x_n}
\newcommand{\betaconv}{\equiv_{\beta}^1}
\newcommand{\etaconv}{\equiv_{\eta}^1}
\newcommand{\ap}[1]{\texttt{ap}(#1)}

\newcommand{\partition}{\;|\;}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\True}{\mathtt{True}}
\newcommand{\False}{\mathtt{False}}
\newcommand{\Nil}{\mathtt{Nil}}

\newcommand{\Int}{\mathsf{Int}}
\newcommand{\Bool}{\mathsf{Bool}}

\newcommand{\constraints}{\mathcal{C}}

\newcommand{\mathsfbrackets}[2]{\mathsf{#1(}#2\mathsf{)}}
\newcommand{\mathttbrackets}[2]{\mathtt{#1(}#2\mathtt{)}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Layout Title page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\begin{titlepage}
 \begin{center}
  {\LARGE Eberhard Karls Universität Tübingen}\\
  {\large Mathematisch-Naturwissenschaftliche Fakultät \\
Wilhelm-Schickard-Institut für Informatik\\[4cm]}
  {\huge Bachelor Thesis Computer Science\\[2cm]}
  {\Large\bf  Higher-Order Unification for Data and Codata Types\\[1.5cm]}
 {\large Julia Wegendt}\\[0.5cm]
Date\\[3cm]
{\small\bf Reviewer}\\[0.5cm]
 {\large Prof. Dr. Klaus Ostermann}\\
  {\footnotesize Department of Computer Science\\
	University of Tübingen}
\end{center}
	
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Layout back of title page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty}
\vspace*{\fill}
\begin{minipage}{11.2cm}
\textbf{Wegendt, Julia}\\
\emph{Higher-Order Unification for Data and Codata Types}\\ Bachelor Thesis Computer Science\\
Eberhard Karls Universität Tübingen\\
Period: from-till
\end{minipage}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{roman}
\setcounter{page}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Abstract}

Algorithms for different subsections of higher-order unification problems have been discussed.
I present an untyped calculus with a focus on the data-codata duality, 
and discuss typing for this calculus.

Furthermore, I go over the decidable subproblem of first-order unification and an accompanying algorithm adapted to our syntax.
I explain higher-order unification and the specific problem of pattern unification formally, 
discussing their differences and decidability.

Finally, a framework for an algorithm and the algorithm for the aforementioned calculus and higher-order unification is presented, 
covering all pattern unification problems and those which can be reduced to pattern unification problems.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Acknowledgements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}

Write here your acknowledgements.

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Table of Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.3}
\small\normalsize

\tableofcontents

\renewcommand{\baselinestretch}{1}
\small\normalsize

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Main Part
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{arabic}
\setcounter{page}{1}

\section{Introduction}

In this thesis, I want to present a higher-order unification algorithm for a calculus with a big focus on codata and data. 
This requires you to know what data and codata conceptually are, how they are presented in our syntax and how they function.
You also need to know what higher-order unification is and how it differs from the unification you probably already know.
This is the goal for the next few paragraphs, as well as giving a quick overview of the different parts of this thesis.

\subsection{Data and codata}
While data (as in algebraic data types) is a well-known concept, 
its brother codata is less talked about. Codata is dual to data, meaning (roughly) any concept expressed using data may also be expressed through codata. 
On a high level: While data is concerned with how values are constructed or stored, codata is concerned with how values are destructed or used. 
For instance, look at these different ways to define the same tuple: 

\begin{example}[Two ways to define the same tuple]
    \begin{align*}
        &\mathttbrackets{Tup}{\mathtt{1,5}} \tag*{Data instantiation}\\
        &\excopatmatch{\mathtt{fst \Rightarrow 1}, \mathtt{snd \Rightarrow 5}} \tag*{Codata instantiation}
    \end{align*}     
    While the first instantiation focuses on what we put into the tuple, the second focuses on how we can take its components out of it.   
\end{example}

One form of codata often talked about is streams:
\begin{example}[Streams]
    \begin{align*}
        Trues = \excopatmatch{\mathtt{hd \Rightarrow True, tl \Rightarrow} Trues}
    \end{align*}
    This is the stream that contains an infinite amount of Trues.
    Also notice again that there is an emphasis on how we can take things out of our stream, rather than how it is constructed.
\end{example}
As you can see, we can also instantiate infinite codata. 
In our syntax, codata is also used for function application, 
which is the part of codata you are most likely the most familiar with:

\begin{example}[Function application through codata]
    \begin{align*}
        id = \excopatmatch{\ap{x}\Rightarrow x}
    \end{align*}
    This is the identity function, which is implemented through mapping a
    function applicator on the variable $x$ to $x$.
\end{example}
    
\subsection{Higher-Order Unification}
Most people probably first encounter unification problems in school, when solving simple math problems.
It is also necessary in a few important applications in computer science: 
Type checking and type inference, in proof assistants, as well as more particular fields like logic programming and computational linguistics.

While unification means solving equations with unknown variables, there are two major types of unification: First-order unification and higher-order unification.
For the type systems in most typed programming languages, first-order unification is enough. It is only when more complex types are introduced, 
like in the interactive theorem prover Coq or in the programming language Agda, higher-order unification is required.

\begin{example}[First-order unification problems]
    \begin{align*}
        &\mathtt{5} \equiv \alphaunifvar\tag{1}\\
        &\mathtt{True \equiv False}\tag{2}\\
        &\mathsf{List(\alphaunifvar)\equiv List(\betaunifvar)}\tag{3}\\
        &\mathtt{Tup(1,\alphaunifvar)}\equiv \betaunifvar \tag{4}
    \end{align*}
    Each of the given equations can be seen as their own unification problem.
    The first has one obvious solution $[\mathtt{5/\alphaunifvar}]$, so substituting $\mathtt{5}$ for $\alphaunifvar$ (even though there are multiple solutions even here).
    The second is a unification problem without unification variables, and has no solution.
    The third and forth are both unification problems with 2 unification variables, and have many solutions.
\end{example}
Note that unification problems may also have multiple equations!

\begin{example}[Higher-order unification problems]\label{sec:mgu}
    In this first example, we are looking for a function that when applied to $\mathtt{5}$, evaluates to $\mathtt{5}$:
    \begin{align*}
        &\alphaunifvar.\ap{\mathtt{5}} \equiv \mathtt{5}\tag{1} 
    \end{align*}
    We can think of two obvious solutions for this: $[id / \alphaunifvar]$ and the constant function which always returns \texttt{5}: $[\excopatmatch{\ap{x}\Rightarrow \mathtt{5}} / \alphaunifvar]$.
    As already implied before, sometimes our unification problems have more than one solution, 
    even though we are interested in one particular solution. This is the solution that is more general, meaning the one from which the others can be generated.
    In this case however, there is no one such solution. This can only happen in higher-order unification, but this will be explored more formally below.
    Another example of higher-order unification:
    \begin{align*}
    \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False \Rightarrow 2}}\equiv \mathtt{1} \tag{2}
    \end{align*}
    Here we have an example of pattern matching implementing an if-clause.
    If we were to substitute $[\mathtt{True / \alphaunifvar}]$, the left side would contract to $\mathtt{1}$,
    since the first case matches - solving the equation.
\end{example}

\subsection{Overview}

Firstly, I will explore an (untyped) calculus with a heavy focus on the dualities between data and codata in \ref{sec:The Untyped Calculus ND}.
I will give examples and show how we can use this syntax to define everything we want to define, and point out how the duality plays out in our syntax.
Furthermore, I will introduce some concepts we need so that we can start to play around with terms.

In \ref{sec:Simple Types}, I give examples of how we can type terms we have seen in our examples, 
and present a typing system to categorize many terms with simple types.

To make our way to higher-order unification, I will first discuss first-order unification in \ref{sec:First-Order Unification},
explaining the problem formally and giving an algorithm to solve it.

Then I explain higher-order unification (\ref{sec:Higher-Order Unification}) formally, as well as a more specific unification problem which is easier to solve.

Finally, in \ref{The Algorithm For Higher-Order Unification}, I present the framework for our equations, 
and then go through our algorithm for higher-order unification problems in our calculus.
It is made up of two parts, which I go though in more detail, 
then describing how they fit together to form our algorithm on a high level.

In the end (\ref{sec:Literature review}) is a literature review, going over related works.

\section{The Untyped Calculus ND}\label{sec:The Untyped Calculus ND}

I will be introducing the Untyped Calculus ND, based on .... %TODO: citation 
It may be seen as an extension to the lambda calculus, as function definition is only one application of codata in this calculus,
meaning we can do more than just function application! 

\subsection{Syntax of the Untyped Calculus ND}\label{sec:syntax}

I write $\overline{e}$ to mean a (possibly empty) list of expressions: $\overline{e}= e_1, ..., e_n$.  
Constructors $\constructor$ and destructors $\destructor$ both get such a list of expressions and construct or destruct terms.
A pattern match $\patmatch$ matches an expressio n $e$ against a sequence of clauses, each clause consisting of a constructor and an term $t$.
For a copattern match $\textbf{cocase} \{\overline{d(\overline{x}) \Rightarrow e}\}$
similar rules apply, but instead of constructors, the expression is matched against destructors.

\begin{definition}[Terms of the Calculus ND]
    \begin{align*}
    e,r,s,t ::&=  x,y  \tag*{Variable} \\
        &\partition\constructor \tag*{Constructor} \\
        &\partition\patmatch  \tag*{Pattern match}\\
        &\partition\copatmatch  \tag*{Copattern match}\\
        &\partition\destructor  \tag*{Destructor}
    \end{align*}
    In pattern and copattern matching, every con- or destructor may occur no more than once.
    This is so we don't match multiple clauses (since $\mathttbrackets{Tup}{x_1, x_2}$ and $\mathttbrackets{Tup}{x, y}$ have the same meaning). %TODO: ist das offensichtlich?
\end{definition}
\iffalse$x, y \in$ \textsc{Var}, $\constructor \in$\textsc{CtorDtorName} $\lor$ \textsc{TypeName}, $\destructor \in$\textsc{CtorDtorName}.
\fi %TODO: warum sieht das so scheiße aus??
\subsection{How is this used?}

To get familiar with the syntax and to understand the underlying semantics, 
I will go through some examples and explain what they implement.

We use constructors for types:
\begin{example}[Constructing types with constructors]
    Simple Types like $\mathsf{Int, Bool, ...}$ are constructors on an empty sequence, 
    while composite types of corse require arguments.
    \begin{align*}
        &\mathsf{Int}\tag{1}\\
        &\mathsf{Bool}\tag{2}\\
        &\mathsf{List(Int)}\tag{3}\\
        &\mathsf{Pair(Int, Bool)}\tag{4}
    \end{align*}
    Types are explored in detail and formally later on in ~\ref{sec:Simple Types}.
\end{example}

We of corse also want to build terms for those data types. 
Some terms we build using constructors:

\begin{example}[Instantiating data with constructors]
    \begin{align*}
        &\texttt{True, False}\tag{1}\\
        &\mathtt{Tup(1,2)}\tag{2}\\
        &\mathtt{Date(27, 08, 2005)}\tag{3}\\
        &\mathtt{Cons(True, Cons(False, Nil))}\tag{4}              
    \end{align*}
    As you can see from the last example, we can use constructors to define recursive data types.
    Numbers for instance, are also implemented recursively as Peano numbers. 
    I use actual numbers as a shorthand throughout this thesis, though.
\end{example}

When we have instantiated terms, we also need a way to take them apart. 
For some terms, this can be done using pattern matching:

\begin{example}[Taking apart data using pattern matching]
    \begin{align*}
        &\True.\expatmatch{\mathtt{True\Rightarrow False, False\Rightarrow True}}\tag{1}\\
        &\mathtt{Tup(1,2)}.\expatmatch{\mathttbrackets{Tup}{x, y}\Rightarrow x}\tag{2}\\
        &\mathtt{Cons(True, Cons(False, Nil))}.\expatmatch{\mathttbrackets{Cons}{x, y}\Rightarrow x}\tag{3}   
    \end{align*}
    This is the pattern matching you probably know. 
    We will see later (\ref{sec:betaconv}) how these get evaluated to $\False$, $\mathtt{1}$ and $\True$, respectively.
\end{example}

Since we use pattern matching to deconstruct data like Booleans, naturally we can also use it to implement conditionals:
\begin{example}[Representing conditionals with pattern matching]
    \begin{align*}
        &t.\expatmatch{\True\Rightarrow e_1, \False\Rightarrow e_2} \tag{1}\\
        &e.\expatmatch{\mathttbrackets{Cons}{\True, x}\Rightarrow \True, 
        \mathtt{Nil\Rightarrow False}, \mathttbrackets{Cons}{x, y}\Rightarrow\False}\tag{2}
    \end{align*}
The first example is equivalent to if $t$ $e_1$ else $e_2$. The second expression tests whether a given list starts with $\True$.
\end{example}

Now this is where codata comes into play. Until now, we have seen how to instantiate data, 
and how to take it apart. This is how to instantiate codata:

\begin{example}[Instantiating codata with copattern matching]
    \begin{align*}
        &\excopatmatch{\mathtt{fst\Rightarrow 1, snd\Rightarrow 2}}\tag{1}\\
        TrafficLight =& \excopatmatch{\mathtt{Green \Rightarrow Yellow, Yellow\Rightarrow, Red\Rightarrow Green}}\tag{2}\\
        Trues =& \excopatmatch{\mathtt{fst \Rightarrow True, snd\Rightarrow}Trues}\tag{3} 
    \end{align*}
    We have seen examples like this in the introduction; the first one is a tuple with the values $\mathtt{1}$ and $\mathtt{2}$.
    In the second, we assume that there are destructors (on an empty sequence) for an algebraic data type that implements the colors of a traffic light.
    Notice again, that we could have defined our traffic light as what colors it displays (using a sum type), but chose to focus on how these colors change, 
    what our traffic light \textbf{does}.
    The third example is the infinite stream of $Trues$.
\end{example}

Now for how to take instantiations of codata apart:

\begin{example}[Taking apart codata using destructors]
    \begin{align*}
        &\excopatmatch{\mathtt{fst\Rightarrow 1, snd\Rightarrow 2}}.\mathtt{fst}\tag{1}\\
        &TrafficLight.\mathtt{Green}\tag{2}\\
        &id.\ap{\mathtt{5}}\tag{3}
    \end{align*}
    These examples do exactly what you think they do: We select the first value of the tuple, turn our traffic light Yellow, 
    and apply the identity function on $\mathtt{5}$, respectively.
    We will see how these examples resolve to $\mathtt{1}$, $\mathtt{Yellow}$ and $\mathtt{5}$ in \ref{sec:betaconv}.
\end{example} 

You might have noticed already that there is no option for function definition in the syntax of our calculus.
This is because we can use copattern matching to implement function definitions:
\begin{example}[Defining functions using copattern matching]
    \begin{align*}
        &id = \excopatmatch{\ap{x}\Rightarrow x}\tag{1}\\
        &\excopatmatch{\ap{x}\Rightarrow (\ap{y}\Rightarrow x)} \tag{2} \\
        &\excopatmatch{\ap{x}\Rightarrow (\ap{y}\Rightarrow y)} \tag{3}
    \end{align*}
    The first example actually implements the identity function. 
    The second and third examples implement a functions that take two arguments, ignore one and give back the other (the first and second, respectively).
\end{example}

\subsection{Dualities in data and codata}
You might have wondered why in the syntax definition, copatterns are defined first, even though they need destructors to function, which are defined only afterwards.
The reason becomes clear when rereading the titles of the definitions:
Constructors are used to instantiate data, while pattern matching is used to take apart data.
Copattern matching is used to instantiate codata, while destructors are used to take apart codata.
The order of the syntax definitions emphasizes these conceptual dualitites:
Conceptually, data is dual to codata, but this is only because constructors are conceptually dual to copattern matching, 
and pattern matching is conceptually dual to destructors.
One more clean example to drive this point home:  

\begin{example}[Dualities in a tuple defined in two ways]
    \begin{align*}
        \begin{tabular}{|l | l | l|}
            \hline
            & data & codata\\
            \hline
            instantiate & $\mathtt{Tup(1,2)}$&  $\excopatmatch{\mathtt{fst\Rightarrow 1, snd\Rightarrow 2}}$\\
            take apart & $.\expatmatch{\mathttbrackets{Tup}{x, y}\Rightarrow x}$ & $.\mathtt{fst}$\\
            example & $\mathtt{Tup(1,2)}.\expatmatch{\mathttbrackets{Tup}{x, y}\Rightarrow x}$&$\excopatmatch{\mathtt{fst\Rightarrow 1, snd\Rightarrow 2}}.\mathtt{fst}$\\
           \hline
       \end{tabular}
    \end{align*}        
\end{example} %TODO: overfull hbox :/

\subsection{Free Variables, Substitutions, Contexts}

\begin{definition}[Free Variables]
    The set of free variables of a term $e$ is FV($e$). A term is closed if this set is empty.
    Free Variables are defined recursively over the structure of terms as follows:
    \begin{align*}
        \begin{split}
            \FV{x} &:= \{x\}\\
            \FV{\exconstructor{\listofexpr}} &:= \FV{e_1} \cup \dots\cup \FV{e_n}\\
            \FV{\patmatch} &:= \FV{e}\cup (\FV{e_1}\backslash\overline{x})\cup \dots\cup (\FV{e_n}\backslash\overline{x})\\
            \FV{\copatmatch} &:= (\FV{e_1}\backslash\overline{x})\cup \dots\cup (\FV{e_n}\backslash\overline{x})\\
            \FV{\exdestructor{e_1,...e_n}} &:= \FV{e}\cup FV(e_1) \cup \dots \cup \FV{e_n}    
        \end{split}
    \end{align*}
%TODO: explanation
\end{definition}


\begin{definition}[Substitution]
    A simultaneous substitution $\sigma$ of the terms $\listofexpr$ for the distinct variables $\listofvar$ is defined as follows:
    \begin{align*} %TODO: geht das mit dem distinct da oben unter?
        \sigma ::= [\listofexpr \backslash\listofvar]
    \end{align*}
\end{definition}

The set of variables for which the substitution is defined is called the domain.
The set of free variables which appear in the substitution is called the range. %TODO: formulierung ausschmücken?

\begin{definition}[Domain and Range of a Substitution]
    The definitions of Domain and Range of a Substitution are as follows:
    \begin{align*}
        \texttt{dom}([\listofexpr/\listofvar]) &:= \{\listofvar\}\\
        \texttt{rng}([\listofexpr/\listofvar]) &:= \FV{e_1}\cup \dots\cup\FV{e_n}
    \end{align*}
\end{definition}

What is actually interesting is what happens when we apply a substitution to an expression:

\begin{definition}[Action of a Substitution]
    The action of a substitution $\sigma$ on a term $e$, written as $e \sigma$ and is defined as follows:
    \begin{align*}
        x[\listofexpr / \listofvar] :=& e_i \quad(\text{if } x=x_i) \\
        y\sigma :=& y\quad (\text{if }y\notin \texttt{dom}(\sigma))\\
        (\exconstructor{\listofexpr})\sigma :=& \exconstructor{e_1 \sigma,\dots , e_n \sigma}\\
        (\patmatch)\sigma:=& (e\sigma).\textbf{case}\{\overline{K(\overline{y}\Rightarrow (e\sigma')\sigma)}\}\\
        (\copatmatch)\sigma:=& \excopatmatch{\overline{(d(\overline{y})\Rightarrow(e\sigma')\sigma)}}\\
        (\exdestructor{\listofexpr}) \sigma:=& (e \sigma).d(e_1 \sigma,\dots , e_n \sigma)
    \end{align*} 
    Where $\sigma'$ is a substitution that ensures that we don't bind new variables: 
    $\sigma'$ has the form $[y_1,\dots, y_n/\listofvar]$ and all $y_i$ are fresh for both the domain and the range of $\sigma$.
\end{definition}


Sometimes we need more than one substitution, or rather want to attach substitutions together.
This process is calles composition of substitutions: $\sigma = \sigma_2 \circ \sigma_1$, which is equivalent to first applying the substitution $\sigma_1$, then the substitution $\sigma_2$.

\begin{definition}[Composition of Subsitutions]
    Given two substitutions
    \begin{align*}
        \sigma_1 := [\listofexpr/\listofvar],\qquad \sigma_2 := [t_1,\dots,t_m/y_1,\dots,y_m],
    \end{align*}
    composition is defined as:
    \begin{align*}
        \sigma_2\circ\sigma_1 := [e_1\sigma_2,\dots,e_n\sigma_2,t_j,\dots,t_k/\listofvar,y_j,\dots,y_k]
    \end{align*}
    Where $j, \dots, k$ ist the greatest sub-range of indices $1,\dots,m$ such that none of the variables $y_j$ to $y_k$ is in the domain of $\sigma_1$. 
\end{definition}

\begin{definition}[Idempotency]
    A substitution $\sigma$ is idempotent, iff. $\sigma \circ \sigma = \sigma$.
    Concretely, this means that it doesn't matter how often we apply a substitution to a given problem.
\end{definition}

Consider these two examples of an idempotent substitution, and one that is not idempotent:
\begin{example}[Idempotency]
    $[\excopatmatch{\ap{x}\Rightarrow x} / y]$ is idempotent, since:
    \begin{align*}
        &[\excopatmatch{\ap{x}\Rightarrow x} / y] \circ [\excopatmatch{\ap{x}\Rightarrow x} / y] \tag{1}\\
        =& [\excopatmatch{\ap{x}\Rightarrow x}[\excopatmatch{\ap{x}\Rightarrow x} / y] / y] \\
        =& [\excopatmatch{\ap{x}\Rightarrow x} / y] 
    \end{align*}
On the other hand, the substitution $[\excopatmatch{\ap{y}\Rightarrow x} / x]$ is not idempotent, since:
\begin{align*}
    &[\excopatmatch{\ap{y}\Rightarrow x} / x] \circ [\excopatmatch{\ap{y}\Rightarrow x} / x]\tag{2}\\ 
    =& [\excopatmatch{\ap{y}\Rightarrow x}[\excopatmatch{\ap{y}\Rightarrow x} / x] / x] \\
    =& \excopatmatch{\ap{y}\Rightarrow (\ap{y}\Rightarrow x) } \neq [\excopatmatch{\ap{y}\Rightarrow x} / x]
\end{align*}

\end{example}

\begin{definition}[More general] 
    A substitution $\sigma$ is more general than a substitution $\theta$, iff. there exists a mapping $\tau$, such that: $\theta = \tau \circ \sigma$.
\end{definition}

For example, in the following unification problem, we are trying to substitute types for two unification variables:
\begin{example}[More general]
    \begin{align*}
        \mathsf{List}(\alphaunifvar) &\equiv \mathsf{List}(\betaunifvar)\\
        \alphaunifvar &\equiv \mathsf{Int}
    \end{align*}
    One solution might be: $\theta = [\mathsf{Int, Int / \alphaunifvar, \betaunifvar}]$, 
so substituting $\mathsf{Int}$ for both unification variables.
The more general solution is $\sigma = [\mathsf{Int,\alphaunifvar / \alphaunifvar, \betaunifvar}]$, however.
This is because there exists a mapping $\tau = [\mathsf{Int / \alphaunifvar}]$, such that:
\begin{align*}
    \tau \circ \sigma &= [\mathsf{Int / \alphaunifvar}]\circ[\mathsf{Int,\alphaunifvar / \alphaunifvar, \betaunifvar}] \\
    &= [\mathsf{Int[Int/\alphaunifvar], \alphaunifvar[Int/\alphaunifvar] / \alphaunifvar, \betaunifvar]= [Int, Int / \alphaunifvar, \betaunifvar]= \theta}
\end{align*}    
\end{example}
%TODO: das ist ein vorgriff! diese sektion doch weiter runter schieben und neues beispiel?

\subsection{Conversion}

\begin{definition}[Beta-conversion] \label{sec:betaconv}
    A single step of beta-conversion $e_1 \betaconv e_2$ is defined as follows:
    \begin{align*}
        \excopatmatch{\dots,d(\overline{x}) \Rightarrow e, \dots}.d(\overline{e})
        \betaconv & e[\overline{e}/\overline{x}]  
        \tag{$\beta$-codata}\\
        \constructor.\textbf{case}\{\dots,\exconstructor{\overline{x}}\Rightarrow e,\dots\}
        \betaconv & e[\overline{e}/\overline{x}] 
        \tag{$\beta$-data}
    \end{align*}
    We require that the constructor $K(\overline{e})$ and the constructor $\exconstructor{\overline{x}}$ have the same number of arguments.
    This, in short ensures that we don't generate stuck terms, i.e. terms that can't be evaluated.
\end{definition}
Consider these examples of beta-conversion:
\begin{example}[Beta-conversion]
    \begin{align*}
        \excopatmatch{\ap{x}\Rightarrow \True}.\ap{x}&\betaconv \True[x/x] = \True 
        \tag{1}\\
        \excopatmatch{\ap{y}\Rightarrow y}.\ap{x} &\betaconv y[x/y]= x
        \tag{2}\\
        \True.\expatmatch{\False\Rightarrow \True, \True \Rightarrow \False}&\betaconv\False
        \tag{3}\\
        \excopatmatch{\mathtt{fst\Rightarrow 1, snd\Rightarrow 2}}.\mathtt{fst}\betaconv \mathtt{1}
        \tag{4}
    \end{align*}
    Intuitively, beta-conversion means not only function application but also the reduction under pattern or copattern matching. %TODO: macht das so sinn? sollte ich das mehr erklären? sind die beispiele hier überhaupt gut?        
\end{example}

\begin{definition}[Eta-Conversion for codata]
    A single step of eta-conversion $e_1 \etaconv e_2$ is defined as follows:
    \begin{align*}
        \excopatmatch{\overline{d(\overline{x})\Rightarrow e.d(\overline{x})}}
        \etaconv & e \quad (\text{if } \overline{x}\notin \FV{e}) \tag{$\eta$-codata}
    \end{align*}
    The expression $e$ needs to be the same in all the different clauses!
\end{definition}

Here are some examples of eta-conversion:
\begin{example}[Eta-conversion for codata]
    \begin{align*}
        \excopatmatch{\ap{y}\Rightarrow id.\ap{y}}&\etaconv id\tag{1}\\
        \excopatmatch{\mathtt{fst\Rightarrow} specificPair.\mathtt{fst}, \mathtt{snd\Rightarrow}specificPair.\mathtt{snd}}&\etaconv e  \tag{2}\\      
        \text{where } specificPair = \expatmatch{\mathtt{fst\Rightarrow 1, snd\Rightarrow 2}}
    \end{align*} %TODO: besserer name???
    The first example says: The function that takes a term and applies the identity function to it, does the same thing as the identity function.
    In the second example, we have a pair that contains the first value of a specific pair, and the second value of a specific pair, which is obviously just that specific pair.    
\end{example}    
    
\section{Simple Types}\label{sec:Simple Types}

We want to be able to use the unification algorithm to discern types for given terms. 
To start, let's look at some examples terms and their types:

\begin{example}[Simple types]
    \begin{align*}
        \mathtt{5}&: \Int\\
        \mathtt{False}&: \Bool\\
        \mathtt{Tup(1,True)}&: \mathsf{Pair(Int, Bool)}\\
        \excopatmatch{\mathtt{fst \Rightarrow 1, snd\Rightarrow True}}&: \mathsf{LPair(Int, Bool)}\\
        \mathtt{Cons(1, Cons(2, Nil))}&: \mathsf{List(Int)}\\
        \excopatmatch{\ap{x}\Rightarrow x.\expatmatch{\mathtt{True\Rightarrow False, False \Rightarrow True}}}&: \mathsf{Bool \rightarrow Bool}\\
        \mathtt{Trues} = \excopatmatch{\mathtt{hd \Rightarrow True, tl \Rightarrow Trues}}&: \mathsf{Stream(Bool)}\\
    \end{align*}    
\end{example}

\subsection{Typing Rules}
To be able to give our examples a formal basis, we will introduce typing rules. %TODO: ich mag die satzstruktur nicht
These will not cover all the types one could construct using our calculus, 
but instead all the basic composite types.
This will be enough to construct examples covering each case we want to consider,
and hopefully give an idea how to construct any type.

When writing $\Gamma\vdash t:\tau$, we mean that from the variables and their types in $\Gamma$, 
we can deduce that $t$ has the type $\tau$.


%% variables + booleans
\begin{minipage}{0.2\textwidth}
    \begin{prooftree}
        \AxiomC{$x:\tau \in \Gamma$}\RightLabel{\textsc{Var}}
        \UnaryInfC{$\Gamma\vdash x:\tau$}
    \end{prooftree}        
\end{minipage}
\begin{minipage}{0.35\textwidth}
    \begin{prooftree}
        \AxiomC{}\RightLabel{\textsc{True}}
        \UnaryInfC{$\Gamma\vdash\mathtt{True}:\mathsf{Bool}$}  
    \end{prooftree}
\end{minipage}
\begin{minipage}{0.35\textwidth}
    \begin{prooftree}
        \AxiomC{}\RightLabel{\textsc{False}}
        \UnaryInfC{$\Gamma\vdash\mathtt{False}:\mathsf{Bool}$}  
    \end{prooftree}
\end{minipage}

%% tuple
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t_1:\tau_1 \Gamma \vdash t_2:\tau_2 $}\RightLabel{\textsc{Tup}}
    \UnaryInfC{$\Gamma \vdash \mathtt{Tup(}t_1,t_2\mathsf{)}: \mathsf{Pair(}\tau_1, \tau_2\mathsf{)}$}    
\end{prooftree}
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t: \mathsf{Pair(}t_1,t_2\mathsf{)}$}\RightLabel{\textsc{Case-Pair}}
    \AxiomC{$\Gamma, x\vdash:\tau_1, y: \tau_2, t':\tau' $}
    \BinaryInfC{$\Gamma \vdash t.\expatmatch{\mathtt{Tup(}t_1,t_2\mathsf{)}\Rightarrow t'}:\tau' $}    
\end{prooftree}


%% nil + cons
\begin{minipage}{0.45\textwidth}
    \begin{prooftree}
        \AxiomC{}\RightLabel{\textsc{Nil}}
        \UnaryInfC{$\Gamma\vdash \Nil:\mathsf{List(\tau)}$}
    \end{prooftree}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t_1:\tau$}\RightLabel{\textsc{Cons}}
    \AxiomC{$\Gamma\vdash t_2:\mathsf{List(\tau)}$}
    \BinaryInfC{$\Gamma\vdash \mathtt{Cons(\mathnormal{t_1,t_2})}:\mathsf{List(\tau)}$}
\end{prooftree}
\end{minipage}

%% case-list
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t:\mathsf{List(\tau')}$}\RightLabel{\textsc{Case-List}}
    \AxiomC{$\Gamma\vdash t_1:\tau$}
    \AxiomC{$\Gamma, y:\tau',z:\mathsf{List(\tau')}\vdash t_2:\tau$}
    \TrinaryInfC{$\Gamma\vdash t.\expatmatch{\Nil\Rightarrow t_1, \mathtt{Cons(\mathnormal{y,z})}\Rightarrow t_2}:\tau$}
\end{prooftree}

%% stream
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t_1:\tau$}\RightLabel{\textsc{Stream}}
    \AxiomC{$\Gamma\vdash t_2:\mathsf{Stream(\tau)}$}
    \BinaryInfC{$\Gamma\vdash \excopatmatch{\mathtt{hd}\Rightarrow t_1, \mathtt{tl}\Rightarrow t_2}: \mathsf{Stream(\tau)}$}
\end{prooftree}

%% hd + tl 
\begin{minipage}{0.45\textwidth} 
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t:\mathsf{Stream(\tau)}$}\RightLabel{\textsc{Hd}}
    \UnaryInfC{$\Gamma\vdash t.\mathtt{hd}:\tau$}    
\end{prooftree}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t:\mathsf{Stream(\tau)}$}\RightLabel{\textsc{Tl}}
    \UnaryInfC{$\Gamma\vdash t.\mathtt{tl}:\mathsf{Stream(\tau)}$}
\end{prooftree}
\end{minipage}

%%lazypair
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t_1:\tau_1$}\RightLabel{\textsc{Lpair}}
    \AxiomC{$\Gamma\vdash t_2:\tau_2$}
    \BinaryInfC{$\Gamma\vdash \excopatmatch{\mathtt{fst}\Rightarrow t_1, \mathtt{snd} \Rightarrow t_2}:\mathsf{LPair(\tau_1, \tau_2)}$}
\end{prooftree}

%% fst + snd
\begin{minipage}{0.45\textwidth}
    \begin{prooftree}
        \AxiomC{$\Gamma\vdash t:\mathsf{Lpair(\tau_1,\tau_2)}$}\RightLabel{\textsc{Fst}}
        \UnaryInfC{$\Gamma\vdash t.\mathtt{fst}:\tau_1$}
    \end{prooftree}    
\end{minipage}
\begin{minipage}{0.45\textwidth}
    \begin{prooftree}
        \AxiomC{$\Gamma\vdash t:\mathsf{Lpair(\tau_1,\tau_2)}$}\RightLabel{\textsc{Snd}}
        \UnaryInfC{$\Gamma\vdash t.\mathtt{snd}:\tau_2$}
    \end{prooftree}    
\end{minipage}

%% function application and function abstraction (?)
\begin{prooftree}
    \AxiomC{$\Gamma\vdash t_1:\tau_1\rightarrow\tau_2$}\RightLabel{\textsc{App}}
    \AxiomC{$\Gamma\vdash t_2:\tau_1$}
    \BinaryInfC{$\Gamma\vdash t_1.\ap{t_2}:\tau_2$}
\end{prooftree}
\begin{prooftree}
    \AxiomC{$\Gamma,x: \tau_1\vdash t:\tau_2$}\RightLabel{\textsc{Fun}}
    \UnaryInfC{$\Gamma\vdash\excopatmatch{\ap{x}\Rightarrow t}:\tau_1\rightarrow \tau_2$}
\end{prooftree}    


\section{First-Order Unification}\label{sec:First-Order Unification}

We want to slowly make our way to higher-order unification, and thus touch on simpler problems first. 
Furthermore, we need a couple concepts more to be able to talk about unification problems and describe our algorithm. 

A unification problem is described by a set of equations with expressions on each side $\overline{e\equiv e}$ containing unknown unification variables $\alphaunifvar_1, \alphaunifvar_2, \betaunifvar...$, 
where our goal is to find a simultaneous substitution $[\listofexpr / \alphaunifvar_1, \dots, \alphaunifvar_n]$ which substitutes expressions for unification variables, 
such that the sides of the given equations are the same (respectively). 

\begin{definition}[Solution]
    A solution to a given unification problem is described by a simultaneous substitution $[\listofexpr / \alphaunifvar_1, \dots, \alphaunifvar_n]$
    which when applied to the problem solves it, i.e. makes the sides of the equations equal.
\end{definition}
    
\begin{definition}[Most General]
    A solution is the most general unifier (mgu), iff. it is more general than all other solutions.
\end{definition} 

Now let's take a look at first-order unification.
Even though first-order unification is a pretty limited subproblem of higher-order unification, 
in many application it is all that is needed - most simple types can be constructed from first-order terms. 

Consider some examples of first-order unification. On the left is the problem, on the right its solution or $\emptyset$ if there is no solution.
\begin{example}[First-order unification]
    \begin{align*}
        \alphaunifvar &\equiv \True &\sigma_1 &= [\True/\alphaunifvar]\tag{1}\\
        \True &\equiv \False &\sigma_2 &= \emptyset\tag{2}\\
        \Int \rightarrow \alphaunifvar &\equiv \Int \rightarrow \Bool &\sigma_3 &= [\Bool/\alphaunifvar]\tag{3}\\
        \alphaunifvar&\equiv \mathsf{List(\alphaunifvar)} &\sigma_4 &= \emptyset\tag{4}
    \end{align*}
    The forth example is an instance of a failing ~\ref{a}.
\end{example}

\begin{definition}[First-order unification]
\begin{align*}
    e,r,s,t ::&= \alphaunifvar, \betaunifvar \tag*{Unification variable}\\
    &\partition x\tag*{Variable} \\
    &\partition \constructor \tag*{Constructor}
\end{align*}
\end{definition}

\begin{theorem}[Decidability of First-Order Unification]
    For first-order unification, there exists an algorithm on equations $\overline{e\equiv e}$, which always terminates, and returns the solution if there exists one. 
    In particular, this solution is always a mgu (i.e. if there is a solution, then there always exists a most general one).
\end{theorem}%TODO: citation

\begin{definition}[Unification algorithm for First-Order Unification ]
    $\perp$ is the symbol for fail.
    The algorithm is defined by non-deterministically applying the below rules:
    \begin{align*}
        E & \cup \{ e \equiv e\} \Rightarrow E \tag{delete}\\
        E & \cup \{\exconstructor{e_1 ... e_n} \equiv \exconstructor{t_1 ... t_n}\} \Rightarrow E \cup \{e_1 \equiv t_1, ... e_n \equiv t_n\}\tag{decompose}\\
        E & \cup \{K_1(e_1,...e_n) \equiv K_2(t_1, ..., t_m)\}  \Rightarrow \perp \quad\text{if $K_1 \neq K_2$ or if $n\neq m$} \tag{conflict}\\
        E & \cup \{e \equiv \alphaunifvar\} \Rightarrow E \cup \{\alphaunifvar \equiv e\}\tag{swap}\\ 
        E & \cup \{\alphaunifvar \equiv e\} \Rightarrow E[e/\alphaunifvar] \cup \{\alphaunifvar \equiv e\} \quad\text{if $\alphaunifvar \in E$ and $\alphaunifvar \notin e$}  \tag{eliminate}\\
        E & \cup \{\alphaunifvar \equiv K(e_1,...e_n)\}\Rightarrow \perp \quad \text{if $\alphaunifvar \in e_1,...e_n$} \tag{occurs check} \label{a}
    \end{align*}
\end{definition}

This algorithm is based on the version presented by Martelli and Montanari in \cite{10.1145/357162.357169},
adapted to our syntax.

\section{Higher-Order Unification}\label{sec:Higher-Order Unification}

\begin{definition}[Higher-Order Unification]
    \begin{align*}
        e,r,s,t  ::&= \alphaunifvar\sigma, \betaunifvar\sigma \tag*{Unification Variables with substitution}\\ %TODO: name? 
            &\partition x  \tag*{Variable} \\
            &\partition\constructor \tag*{Constructor} \\
            &\partition\patmatch  \tag*{Pattern match}\\
            &\partition\copatmatch  \tag*{Copattern match}\\
            &\partition\destructor  \tag*{Destructor}
    \end{align*}
\end{definition}

Note that this is encompassed our syntax described in \ref{sec:syntax},
but with the addition of unification variables with substitutions. 
To illustrate the need for this substitution, look at what problem arises when omitting the substitution:
\begin{example}[Substitutions on unification variables] 
    Consider this example:
    \begin{align*}
        \excopatmatch{\ap{x}\Rightarrow\alphaunifvar}.\ap{y}\equiv\betaunifvar
    \end{align*}
    If you focus on the left side, you might notice that there is a redex. What happens if we reduce it?
    \begin{align*}
        \excopatmatch{\ap{x}\Rightarrow\alphaunifvar}.\ap{y}\betaconv\alphaunifvar[y/x]    
    \end{align*}     
    If we now found the solution $\sigma = [x/ \alphaunifvar]$ through another equation, we would actually need to substitute $y$ for $x$ in $\alphaunifvar = x$!
\end{example}

This motivates our need for substitutions on unification variables: 
Since we don't know what the unification variable will solve to, we might need to perform a substitution on it later.
Note that this is \textit{not} possible in first-order unification, since we don't create substitutions through redexes! 
When the substitution is trivial, we may  write $\alphaunifvar, \betaunifvar$ instead.

In higher-order unification in contrast to in first-order unification, we are not interested in syntactic equality, but want a broader set of terms to be equal to one another.
Depending on the type of unification problem one wants to solve, they may want to only include beta-equality or both 
beta- and eta-equality.
This further motivates the use of the symbol $\equiv$ so far. Whereas in first-order unification it just stands for syntactic equality, 
in higher-order unification, I use it to mean syntactically equality, beta-equality or eta-equality.
This essentially means that two terms are equivalent if they are equivalent after function application, (co-)pattern matching evaluation, and/or are equivalent externally.

In the introduction (\ref{sec:mgu}), I alluded to the fact that many problems have multiple solutions, but we prefer a certain, most general solution.
Let's next consider a familiar example again:
\begin{example}[No more general solution]
    \begin{align*}
        \alphaunifvar.\ap{5} \equiv \texttt{5}
    \end{align*}
    This problem has multiple solutions:
    \begin{align*}
        \sigma_1 = [\excopatmatch{\ap{x}\Rightarrow x} / \alphaunifvar]\tag*{identity function}\\
        \sigma_2 = [\excopatmatch{\ap{x}\Rightarrow 5} / \alphaunifvar]\tag*{constant function, always returns 5}
    \end{align*}
    In this case, neither solution is more general. Why becomes apparent when You take a look at the action of a substitution again. 
Let's say we wanted to find a substitution to apply after the identity function to get the constant function. 
When applying a substitution to a copattern match, we substitute new variables for our bound variables to not accidentally change the meaning of the term.
This prevents us from changing our function. 
Since neither solution is more general (and there also is no other solution that is more general),
there is no mgu for this problem!
\end{example}

Unification problems having no mgu is specific to higher-order unification, since in first-order unification we don't look at 
higher-order terms and thus don't have that problem of not changing variables.

\begin{theorem}[Decidability of higher-order unification]
    Higher-order unification includes unification problems containing higher-order terms (including functions),
    and is covered by our introduced syntax. 
    Higher-order unification is not decidable. This can be proven through reducing Hilbert's tenth problem to the unification problem.%TODO: schmöker citen? oder wird das da auch nicht wirklich gemacht?
\end{theorem} %TODO: citation

\subsection{Pattern Unification}

Pattern unification, also sometimes called the pattern fragment
is a subsection of higher-order unification, with its path to a solution being similarly simple as the one to first-order unification.
It was described first by Miller in \cite{10.1093/logcom/1.4.497}.
Since we are dealing with what amounts to an extension to the lambda calculus, we need to extend our definition to more than just function applications. 
This means that (conceptually) the pattern fragment described by Miller is a subset of our definition.

\begin{definition}[Pattern]
    A pattern is any term $p$ 
    \begin{align*}
        p ::= \alphaunifvar, \betaunifvar \partition p.d(\listofvar)
    \end{align*}
    where it holds that all $\listofvar$ are distinct variables.
\end{definition}

Consider some examples for patterns. On the left is the pattern unification problem and on the right its solution:
\begin{example}[Pattern]
    \begin{align*}
        \alphaunifvar.\ap{x}.\ap{y}&\equiv x &\sigma_1 &= [\excopatmatch{\ap{x}\Rightarrow\excopatmatch{\ap{y}\Rightarrow x}}/\alphaunifvar] \tag{1}\\
        \alphaunifvar.fst &\equiv 2 &\sigma_2 &= [\excopatmatch{\mathtt{fst\Rightarrow 2, snd \Rightarrow}x}/\alphaunifvar] \tag{2}
    \end{align*}   
\end{example}
%TODO: beispiele?

\begin{theorem}[Decidability of Pattern Unification]
    Pattern Unification is decidable. If there exists a solution, there also exists a mgu.
\end{theorem}

Note that first-order unification is not a subset of pattern unification. 
Equations containing first-order terms that don't contain patterns still remain solvable.

The reason there always exists an mgu lies in the constraint we put on our definition: All the variables must be distinct. 
To illustrate this, take the following problem where the variables are \textbf{not} distinct:
\begin{example}[Why distinct variables in patterns?]
    \begin{align*}
        \alphaunifvar.\ap{x}.\ap{x}\equiv x
    \end{align*}  
    We can name two solutions:
\begin{align*}
    \sigma_1 &= [\excopatmatch{\ap{x}\Rightarrow \excopatmatch{\ap{y}\Rightarrow x}}/\alphaunifvar]\\
    \sigma_2 &=[\excopatmatch{\ap{x}\Rightarrow \excopatmatch{\ap{y}\Rightarrow y}}/\alphaunifvar]  
\end{align*}
(Intuitively, the solutions say to select the first or second argument of the function applications, respectively.)
These solutions are equivalent, in that no solution is more general than the other. There exists no mgu for this problem.
The solutions aren't unique because the variables aren't unique.  
\end{example}

%TODO: boring
To be able to talk about the algorithms for solving unification problems, we need another definition: %TODO: umformulieren
\begin{definition}[Normal Form]
    The normal form \textbf{NF} is defined as follows:
    \begin{align*}
        n &::= x \partition \alphaunifvar \partition n.d(\overline{v})\partition n.\textbf{case}\{\overline{K(\overline{x})\Rightarrow v}\}\\
        v &::= n \partition K(\overline{v}) \partition \excopatmatch{\overline{d(\overline{x})\Rightarrow v}}
    \end{align*}
    Terms that satisfy the $n$-definition are called neutral terms, $v$-terms are called values.
\end{definition}
Note that these are the terms that do not contain beta-redexes (a term that can be transformed using a beta-conversion).
This is apparent when one tries to construct terms containing beta-redexes in normal form: 
To construct the first kind of beta-redex: $\constructor.\textbf{case}\{\dots,\exconstructor{\overline{x}}\Rightarrow e\}$,
we start with the term $n.\textbf{case}\{\overline{K(\overline{x})\Rightarrow v}\}$, now wanting to subtitute $K(\overline{e})$ for $n$. 
This is not possible because we are limited to neutral terms, which constructors are not part of.
Similarly with the second kind of beta-redex:
$\excopatmatch{\dots,d(\overline{x}) \Rightarrow e, \dots}.d(\overline{e})$, where we can not substitute the cocase in $n.d(\overline{e})$ because we are limited to neutral terms.
        
This underlines the intuition that neutral terms are terms that cannot be reduced because
they contain an term we can't further evaluate (either a variable or a unification variable) in the front.
Even though we don't have this assurance in values, that is not a problem since values don't contain the building blocks for beta-reductions. 
Some examples of terms in normal form: 
\begin{example}[Normal form]
\begin{align*}
    \alphaunifvar.\ap{\excopatmatch{\ap{x}\Rightarrow x}} \tag{1}\\
    \excopatmatch{\ap{x}\Rightarrow \mathtt{Cons(\mathnormal{x}, Cons(\mathnormal{y}, Nil))}} \tag{2}\\
    \alphaunifvar.\ap{x_1, x, y, z} \tag{3}    
\end{align*}
The first and second example are in normal form and the third is a pattern in normal form.
\end{example}

\begin{theorem}
    If and only if a unification problem has a solution, that solution is the same for the normal form of that problem. 
    The same thing holds for the other way around: If and only if a normal form has a solution, that solution is the same for the extended expression of that normal form. 
\end{theorem}

This is helpful for us, since we only have to consider normal forms in our algorithm.
Concretely: Met with a term, we first reduce it to its normal form, and then apply the steps to find our solution.

Thus, from this point on, we will only be looking at terms that have a normal form. This is enough in most applications. %TODO: ausschmücken?

For a practical example for why normal forms are helpful, consider the following problem:
\begin{example}[Beta-reductions before solving]
    \begin{align*}
        \True.\expatmatch{\True \Rightarrow \alphaunifvar, \False \Rightarrow \mathtt{2}} \equiv \mathtt{3} \tag{1}
    \end{align*}
    has the solution $[\mathtt{3/\alphaunifvar}]$, but this is more obvious after using a beta-reduction on the left side 
    to bring it into normal form:
    \begin{align*}
        &\True.\expatmatch{\True \Rightarrow \alphaunifvar, \False \Rightarrow \mathtt{2}} \betaconv \alphaunifvar\\
        &\implies \alphaunifvar \equiv \mathtt{3}   
    \end{align*}       
Let's take a look at another example of a using beta-conversion:
\begin{align*}
    &\alphaunifvar \equiv \True \tag{2}\\
    &\alphaunifvar.\expatmatch{\mathtt{True\Rightarrow 2, False\Rightarrow 3}} \equiv \betaunifvar
\end{align*}
To find out that $[\mathtt{2}/\betaunifvar]$ is the solution to the second equation, we first need to find out that
$[\True/\alphaunifvar]$ is the solution to the first equation and substitute it in the second.
This is what is called \textbf{dynamic pattern unification}, where we hold off on solving some equations we don't have all the information for yet until we know more. 
 
\end{example}


\section{The Algorithm For Higher-Order Unification}\label{The Algorithm For Higher-Order Unification}

\subsection{Constraints}

Even though higher-order unification is undecidable, in many cases we can of corse still find solutions,
or conclude that there is no solution.
Since we don't only want to describe higher-order unification, but the steps necessary to solve higher-order unification problems,
we need to have a framework of our equations; a framework where we can simplify our equations,
and store what we have found out about our unknown variables. This is where constraints come into play:

\begin{definition}[Constraint]
    \begin{align*}
        C ::= \top \partition \bot \partition \Psi \vdash e\equiv t \tag*{where $\Psi = x_1, ..., x_n$}\\
        \mathcal{C} ::= C \wedge \mathcal{C}
    \end{align*}
    The variables in the Context $\Psi$ need to be distinct. $\Psi$ may also be empty.
\end{definition}
$\Psi \vdash e\equiv t$ means that given the variables $\Psi$, we deduce that $e\equiv t$.
$\Psi$ may contain variables that occur in or are even bound in $e$ or $t$. 
$\Psi$ contains those variables we have seen before and want to remember. This motivates the naming of $\Psi$ as context, as well.
%TODO: too choppy

We formulate our constraints from a given unification problem as follows:
We take each given equation and formulate a constraint $K$ with empty context $\Psi$. We join them using ands:
$\overline{e\equiv t}$ become $\constraints = C_1 \wedge ... \wedge C_n = \, \vdash e_1 \equiv t_1 \wedge ...\wedge \vdash e_n \equiv t_n $  

For clarification, $C \wedge \mathcal{C}$ is the regular logical and thus, 
and $\top \wedge \constraints = \constraints$ as well as $\bot \wedge \constraints = \bot$.
We can also apply substitutions on a set of constraints, which just means applying the substitution to the equation part of all the constraints in the set:
$\constraints \sigma = (C \wedge \constraints')\sigma = C \sigma \wedge \constraints' \sigma  
= \Psi \vdash e\sigma\equiv t\sigma\wedge \constraints'\sigma$. Applying a substitution to $\top$ or $\bot$ changes nothing.

\subsection{Decomposing Constraints}

It is helpful to simplify our equations a bit before starting the unification process.
We do this by taking out redundant equations, spotting clearly contradictory equations or splitting our equations into smaller parts which we can further simplify and unify.

Let's look at some examples. We would like the following to be true:
\begin{example}[There are two kinds of variables!]
    \begin{align*}
    &x \vdash x.\mathtt{fst} \equiv x.\mathtt{fst} &\mapsto& \top
    \tag{1}\\
    &x \vdash x.\mathtt{fst} \equiv x.\mathtt{snd} &\mapsto& \bot 
    \tag{2}\\
    &x,y \vdash x.\mathtt{fst} \equiv y.\mathtt{fst} &\mapsto& \bot 
    \tag{3}
    \end{align*}
    These examples might be confusing at first. Looking at the second example, one might argue that there exists a pair, say $x = \excopatmatch{\mathtt{fst\Rightarrow 2, snd\Rightarrow 2}}$, such that
    $x.\mathtt{fst} = x.\mathtt{snd}$. This is where it is important to remember that we are not looking for solutions to variables like $x$.
    Whereas to our unification variables $\alphaunifvar$ we want to assign any term such that our equations hold, variables $x$ add \textbf{constraints} to our equations. 
    The equation $x \vdash x.\mathtt{fst} \equiv x.\mathtt{snd}$ must hold for \textbf{any} variable $x$, since we cannot choose $x$! 
    Since for $x = \excopatmatch{\mathtt{fst\Rightarrow 2, snd\Rightarrow 3}}$, the equation does not hold, we can simplify to $\bot$.
    %TODO: rework sentence structure
    %TODO: can i explain this better?
    %TODO: write something about the third example?
\end{example}

\begin{example}[Constructors and Destructors]
    \begin{align*}
    &\vdash \mathttbrackets{Cons}{x, \mathtt{Nil}} \equiv \mathttbrackets{Cons}{y, \mathtt{Nil}}&\mapsto& \bot
    \tag{1}\\
    &\vdash \texttt{Cons(}\alphaunifvar, \texttt{Nil)}\equiv\texttt{Cons(}\betaunifvar, \texttt{Nil)}
    &\mapsto&\vdash \alphaunifvar\equiv \betaunifvar
    \tag{2}\\
    &\vdash \mathsf{List(Int)}\equiv \mathsf{List(\alphaunifvar)}
    &\mapsto& \vdash \mathsf{Int}\equiv \alphaunifvar
    \tag{3}\\
    &x \vdash x.\ap{e_1}\equiv x.\ap{e_2} &\mapsto& x\vdash e_1\equiv e_2 
    \tag{4}\\
    &\vdash\mathtt{\alphaunifvar.fst \equiv \betaunifvar.fst}&\mapsto& \vdash\alphaunifvar\equiv \betaunifvar
    \tag{5}
    \end{align*}
\end{example}

\begin{example}[Pattern and Copattern Matching]
    \begin{align*}
    &x\vdash x.\expatmatch{\texttt{T} \Rightarrow e_2, \texttt{F} \Rightarrow e_2} \equiv x.\expatmatch{\texttt{T} \Rightarrow t_1, \texttt{F} \Rightarrow t_2}\footnotemark[1]\tag{1}
    \\ &\mapsto x\vdash  e_1 \equiv t_1\wedge x\vdash e_2\equiv t_2
    \\
    &\vdash \excopatmatch{\ap{x}\Rightarrow \alphaunifvar} \equiv \excopatmatch{\ap{x}\Rightarrow\betaunifvar} 
    &\mapsto&x\vdash \alphaunifvar \equiv \betaunifvar
    \tag{2}\\
    &\excopatmatch{\mathtt{fst \Rightarrow} \mathtt{5}, \mathtt{snd\Rightarrow False}}\equiv \alphaunifvar \tag{3}
    \\ &\mapsto \vdash \mathtt{5}\equiv \alphaunifvar.\mathtt{fst}\wedge \vdash \mathtt{False\equiv\alphaunifvar.snd}
    \end{align*}
\end{example}
\footnotetext[1]{\texttt{T} and \texttt{F} are stand-ins for $\True$ and $\False$.}
Note that all of these examples are in normal form, i.e. they don't contain redexes.
This means the only way for the terms on each side of the equations to be equivalent,
is for them to be equivalent \textbf{syntactically}. 

We want to formulate rules which help us simplify equations like these - equations without beta-redexes.
We want remove redundant constraints, split constraints made up of composite terms to find solutions to their parts,
and spot redundancies to prematurely stop the solving process.
\begin{definition}[Decomposing Constraints]
    \begin{align*}
    &\textbf{Removing redundancy}\\
    &\Psi \vdash x\equiv x&\mapsto_r&\top
    \tag{1}\label{1}\\
    &\Psi \vdash \alphaunifvar \equiv \alphaunifvar&\mapsto_r& \top
    \tag{2}\\
    &\Psi \vdash \exconstructor{} \equiv \exconstructor{} &\mapsto_r& \top 
    \tag{3}\label{3}\\
    &\textbf{Decomposition}\\
    \circ\,&\Psi\vdash K(\overline{e})\equiv K(\overline{t})&
    \mapsto_d&\overline{\Psi\vdash e\equiv t}
    \tag{4}\label{4}\\
    \circ\,&\Psi \vdash e_1.d({\overline{e}})\equiv e_2.d({\overline{t}})
    \\ &\mapsto_d \Psi\vdash e_1 \equiv e_2 \wedge \overline{\Psi\vdash e\equiv t}
    \tag{5}\label{5}\\
    *\,&\Psi \vdash e_1.\expatmatch{\overline{K(\overline{x})\Rightarrow e}}\equiv e_2.\expatmatch{\overline{K(\overline{x})\Rightarrow t}}
    \\ &\mapsto_d \Psi \vdash e_1 \equiv e_2\wedge \overline{\Psi, \overline{x}\vdash e\equiv t}
    \tag{6}\\
    *\, &\Psi\vdash \excopatmatch{\overline{d(\overline{x})\Rightarrow e}}\equiv 
    \excopatmatch{\overline{d(\overline{x})\Rightarrow t}}
    &\mapsto_d& \overline{\Psi, \overline{x}\vdash e \equiv t}
    \tag{7}\label{7}\\
    &\Psi\vdash \excopatmatch{\overline{d(\overline{x})\Rightarrow e}}\equiv t 
    &\mapsto_d& \overline{\Psi,\overline{x} \vdash e \equiv t.d(\overline{x})}
    \tag{8}\label{8}\\ 
    &\Psi\vdash t\equiv \excopatmatch{\overline{d(\overline{x})\Rightarrow e}} 
    &\mapsto_d& \overline{\Psi, \overline{x}\vdash t.d(\overline{x}) \equiv e}
    \tag{9}\label{9}\\
    &\textbf{Eta-reduction}\\
    & \Psi \vdash \excopatmatch{\overline{d(\overline{x})\Rightarrow e.d(\overline{x})}}\equiv t
    &\mapsto_e&\Psi\vdash e\equiv t
    \tag{10}\\
    &\textbf{Removing contradictions}\\ 
    &\Psi \vdash x\equiv y &\mapsto_c& \bot 
    \tag{11}\\   
    &\Psi \vdash K_1() \equiv K_2() &\mapsto_c& \bot 
    \tag{12}\\
    &\text{non-matching *-equation}&\mapsto_c&\bot
    \tag{13}\label{13}\\
    \end{align*}
\end{definition}
Rules 7-9 are taken from \cite{10.5555/2021953.2021960}, adapted to our syntax. 

For equations marked with *, we require the constructors (or destructors) to be equal to one another in each equation. 
We also require them to have the same list of variables as arguments, respectively (i.e. full syntactic equality).
For equations marked with $\circ$, we only require the constructors (or destructors) to be equal to one another in each equation. 
(i.e. no syntactic equality among arguments required).
This is because in *-equations, we expect variables, whereas in $\circ$-arguments, we expect values (since we expect terms in normal form).
In *-equations where the arguments are not equal syntactically, rule \ref{13} applies and we simplify to $\bot$:

\begin{example}[Contradiction in (co-)pattern matching]
    \begin{align}
        &\vdash x.\expatmatch{\mathtt{True\Rightarrow \alphaunifvar, False \Rightarrow \betaunifvar}} 
        \equiv x.\expatmatch{\mathtt{1\Rightarrow \alphaunifvar, 2\Rightarrow\betaunifvar}}
        &\mapsto_c& \bot \tag{1}\\
        &\vdash \excopatmatch{\mathtt{fst\Rightarrow 1, snd\Rightarrow 2}}
        \equiv \excopatmatch{\ap{x}\Rightarrow \mathtt{1}}
        &\mapsto_c& \bot \tag{2}
    \end{align}
\end{example}

Note that \ref{8} (as well as \ref{9} with being just the mirror of \ref{8}) are possible through eta-conversion:
\begin{align*}
    \excopatmatch{\overline{d(\overline{x})\Rightarrow e}}&\equiv t \\
    \excopatmatch{\overline{d(\overline{x})\Rightarrow e}}&\equiv \excopatmatch{\overline{d(\overline{x})\Rightarrow t.d(\overline{x})}}\\
    &\xmapsto{(\ref{7})}_d \overline{e \equiv t.d(\overline{x})}
\end{align*}

\subsection{Unification}
\footnotetext[1]{\texttt{T} and \texttt{F} are stand-ins for $\True$ and $\False$.}

What do we do when we can't simplify anymore? How do we actually find the solutions?
Let's start with an example - take a look at the following unification problem:
\begin{example}[Unification]
    \begin{align*}
        \mathtt{Tup(\alphaunifvar, False)}&\equiv \mathtt{Tup(True, False)}\\
        \betaunifvar&\equiv \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}
    \end{align*}        
    I will demonstrate how we want to solve this, introducing what to do when we found a solution.   
    \begin{align*}
        \constraints &= &\vdash\ & \mathtt{Tup(\alphaunifvar, F)}\equiv \mathtt{Tup(T, F)}&\wedge&
        \vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{T \Rightarrow 1, F\Rightarrow 5}}\footnotemark[1]\\
        &\xmapsto{(\ref{4})}_d &\vdash\ & \mathtt{\alphaunifvar\equiv T}\qquad\wedge\vdash \mathtt{F\equiv F} &\wedge& 
        \vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{T \Rightarrow 1, F\Rightarrow 5}}\\
        &\xmapsto{(\ref{3})}_r &\vdash\ & \mathtt{\alphaunifvar\equiv T} &\wedge& 
        \vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{T \Rightarrow 1, F\Rightarrow 5}}\\
        &\mapsto_u &\vdash\ &\mathtt{\alphaunifvar\equiv T} &\wedge&
        \vdash \betaunifvar\equiv \mathtt{T}.\expatmatch{\mathtt{T \Rightarrow 1, F\Rightarrow 5}}\\
        &\stackrel{\betaconv}{=} &\vdash\ &\mathtt{\alphaunifvar\equiv T} &\wedge&
        \vdash \betaunifvar\equiv \mathtt{1}
    \end{align*}
What do we learn about unification steps from this example? When we have found an assignment to a unification variable, we want to keep it in our constraints!
This is because we sometimes have to substitute that assignment in other constraints that contain the same unification variable.
But this example tells us something else, too: Doing so might introduce redexes!
This means that we might need to reduce terms to normal form again, even after applying unification steps. 
\end{example}

\iffalse
\begin{align*} %overfull
    \constraints &= &\vdash\ & \mathtt{Tup(\alphaunifvar, False)}\equiv \mathtt{Tup(True, False)}&\wedge&
    \vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\xmapsto{(5)}_d &\vdash\ & \mathtt{\alphaunifvar\equiv True}\qquad\wedge\vdash \mathtt{False\equiv False} &\wedge& 
    \vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\xmapsto{(3)}_r &\vdash\ & \mathtt{\alphaunifvar\equiv True} &\wedge& 
    \vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\mapsto_u &\vdash\ &\mathtt{\alphaunifvar\equiv True} &\wedge&
    \vdash \betaunifvar\equiv \mathtt{True}.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\stackrel{\betaconv}{=} &\vdash\ &\mathtt{\alphaunifvar\equiv True} &\wedge&
    \vdash \betaunifvar\equiv \mathtt{1}
\end{align*} 
\begin{align*} %split lines
    \constraints &= \vdash\ \mathtt{Tup(\alphaunifvar, False)}\equiv \mathtt{Tup(True, False)}\\
    &\wedge\vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\xmapsto{(5)}_d \vdash\   \mathtt{\alphaunifvar\equiv True}\qquad\wedge\vdash \mathtt{False\equiv False}\\
    &\wedge\vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\xmapsto{(3)}_r \vdash\   \mathtt{\alphaunifvar\equiv True} \\
    &\wedge\vdash \betaunifvar\equiv \alphaunifvar.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\mapsto_u \vdash\  \mathtt{\alphaunifvar\equiv True}\\
    &\wedge\vdash \betaunifvar\equiv \mathtt{True}.\expatmatch{\mathtt{True \Rightarrow 1, False\Rightarrow 5}}\\
    &\stackrel{\betaconv}{=} \vdash\  \mathtt{\alphaunifvar\equiv True} \\
    &\wedge\vdash \betaunifvar\equiv \mathtt{1}
\end{align*}
\fi

There are very few unification steps: 
\begin{definition}[Unification]
    \begin{align*}
        & e \equiv\alphaunifvar
        &\mapsto_u& \alphaunifvar \equiv e 
        \tag{1}\\
        &\constraints \wedge \Psi \vdash \alphaunifvar \equiv e 
        &\mapsto_u& \constraints[e/\alphaunifvar] \wedge \Psi \vdash \alphaunifvar \equiv e &(\alphaunifvar \notin e)
        \tag{2}\label{unif2}\\
        &\constraints \wedge \Psi \vdash \alphaunifvar \equiv e
        &\mapsto_u& \bot &(\alphaunifvar\in e)
        \tag{3}
    \end{align*}
Step 2 and 3 are the occurs check.
\end{definition}

We should be careful, because there is another thing we could be introducing through unification steps, though:
\begin{example}[Introducing simplifiable constraints through unficiation steps]
    \begin{align*}
        \constraints &=& &\vdash \excopatmatch{\ap{x}\Rightarrow\alphaunifvar}\equiv id 
        &\wedge& &\vdash \betaunifvar.\ap{x}\equiv \alphaunifvar\\
        &\xmapsto{(\ref{8})}_d& &x \vdash \alphaunifvar \equiv id.\ap{x}
        &\wedge& &\vdash \betaunifvar.\ap{x}\equiv \alphaunifvar\\
        &\xmapsto{(\ref{unif2})}_u& &x \vdash \alphaunifvar \equiv id.\ap{x}
        &\wedge& &\vdash \betaunifvar.\ap{x}\equiv id.\ap{x}\\
        &\xmapsto{(\ref{5})}_d& &x \vdash \alphaunifvar \equiv id.\ap{x} 
        &\wedge& &\vdash \betaunifvar \equiv id \wedge \vdash x \equiv x\\
        &\xmapsto{(\ref{1})}_r& &x \vdash \alphaunifvar \equiv id.\ap{x}
        &\wedge& &\vdash \betaunifvar \equiv id
    \end{align*}
    Here, the unification step introduces another constraint we can simplify! 
\end{example}

\subsection{The algorithm}
Having described all the steps of our algorithm, the given examples lead us to a question:
How and when do we apply these steps?

Given a set of equations, we normalize the terms in them and are left with only normal forms. 
For each equation, we formulate a constraint and get the set of constraints through logical ands.
Next, we decompose the constraints, removing redundancies and potentially terminating early since we find contradictions.
We do this non-deterministically, since the order doesn't matter. %TODO: does it really not?

When we can't further simplify, we non-deterministically apply unification steps, beta-reduce our terms and apply decomposition steps.
When we have found an assigned term for each unification variable, we can terminate and give the solution
by formulating a substitution for each assigned term.

This algorithm works on any pattern unification problem, and in particular, any unification problem that can be reduced to a pattern unification problem. %TODO: how?

\section{Literature review}\label{sec:Literature review}


\newpage
In \cite{DBLP:books/el/RV01/Dowek01}, Huet's algorithm is described. %TODO: spelling?



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\addcontentsline{toc}{chapter}{Bibliography}

\bibliographystyle{abbrv}
\bibliography{mylit}
%% Obige Anweisung legt fest, dass BibTeX-Datei `mylit.bib' verwendet
%% wird. Hier koennen mehrere Dateinamen mit Kommata getrennt aufgelistet
%% werden.

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Erklaerung
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}
\section*{Selbständigkeitserklärung}

Hiermit versichere ich, dass ich die vorliegende Bachelorarbeit 
selbständig und nur mit den angegebenen Hilfsmitteln angefertigt habe und dass alle Stellen, die dem Wortlaut oder dem 
Sinne nach anderen Werken entnommen sind, durch Angaben von Quellen als 
Entlehnung kenntlich gemacht worden sind. 
Diese Bachelorarbeit wurde in gleicher oder ähnlicher Form in keinem anderen 
Studiengang als Prüfungsleistung vorgelegt. 

\vskip 3cm

Ort, Datum	\hfill Unterschrift \hfill 


%%% Ende
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

\iffalse
Für wenn wir patterns in mehr kontext behandeln i guess

Some examples of patterns:
\begin{align*}
    \begin{split}
    t_1 &= \excopatmatch{\ap{x}\Rightarrow \alphaunifvar.\ap{x}} \\
    t_2 &= \excopatmatch{\ap{x}\Rightarrow \excopatmatch{\ap{y}\Rightarrow (\alphaunifvar.\ap{y}).\ap{x}}}    
    \end{split}
\end{align*} 
In each of these terms, our unification variable is applied to distinct variables which are bound through  cocases.

However, these terms are not patterns:
\begin{align*}
    \begin{split}
        t_3 &= (\alphaunifvar.\ap{x}).\ap{y}\\
        t_4 &= \excopatmatch{\ap{y}\Rightarrow (\alphaunifvar.\ap{x})}\\
        t_5 &= \excopatmatch{\ap{x}\Rightarrow (\alphaunifvar.\ap{x}).\ap{x}}
    \end{split}
\end{align*}
as they all don't contain exactly one cocase per variable which would bind that variable distinctly.
%TODO: macht das sinn mit dem distinctly? 
\fi
